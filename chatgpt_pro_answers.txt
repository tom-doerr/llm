**TLDR**

* **Yes, one QSFP cable *can* negotiate 200 GbE on DGX Spark — but you won’t see ~200 Gb/s with plain TCP/NCCL-socket.** On Spark, each physical 200 Gb port effectively behaves like **two ~100 Gb “halves”**; **single-flow TCP tops out ~100 Gb/s**, and NCCL **NET/Socket** also tends to land in that range. To get **~190–200 Gb/s aggregate**, you need **RoCE (NCCL NET/IB)** and you must drive **both halves concurrently**. ([NVIDIA Developer Forums][1])
* **Two cables won’t give you 400 Gb/s on Spark in practice.** People testing two cables still report topping out around the same ~200 Gb/s class. Use two cables only for redundancy/experiments, not because you expect 2× throughput. ([NVIDIA Developer Forums][1])
* **For Qwen3‑235B‑A22B, the “max batched throughput” path on 2× Spark is: TRT‑LLM + NVFP4 + proper RoCE.** NVIDIA’s published reference for dual Spark is **23,477 tok/s prefill** and **11.73 tok/s decode** (ISL 2048 / OSL 128 / BS=1). ([NVIDIA Developer][2])
* If you care about **decode-heavy throughput**, add **speculative decoding** (NVIDIA claims up to **2.6×** vs FP8 for Qwen‑235B on dual Spark). ([NVIDIA Developer][3])

---

## 1) Reality check: what “200 GbE with one cable” means on DGX Spark

On DGX Spark, **a single physical 200 GbE port is exposed in Linux as two logical interfaces** (you’ll see the `enp...` *and* the `enP2p...` twin). The key consequence:

* **TCP / NET=Socket:** you usually hit **~100–110 Gb/s** per port because you’re effectively using one “half.” ([NVIDIA Developer Forums][1])
* **RoCE / NET=IB (NCCL using verbs):** if configured correctly, you can drive **both halves** and get **~196–198 Gb/s aggregate**, and NCCL all‑reduce bus bandwidth around **~23–25 GB/s** (≈190–200 Gb/s class). ([NVIDIA Developer Forums][4])

So: **your instinct (“single cable should give 200 Gb/s”) is directionally right, but only if you’re actually using RoCE/verbs + both halves.**

---

## 2) Single-cable setup that actually gets ~200 Gb/s (RoCE)

### Step A — Identify which port you actually cabled

Run on **both** Sparks:

```bash
sudo ibdev2netdev
ip link | egrep 'enp1s0f|enP2p1s0f'
```

You’re looking for the pair where both the `enp...` **and** its `enP2p...` twin show the link up.

Then confirm link speed on the `enp...` side:

```bash
sudo ethtool enp1s0f0np0 | egrep 'Speed|Duplex|Link detected|Lanes'
sudo ethtool enp1s0f1np1 | egrep 'Speed|Duplex|Link detected|Lanes'
```

You want **Speed: 200000Mb/s** and **Link detected: yes**. ([NVIDIA Developer Forums][4])

---

### Step B — Use static IPs (don’t rely on 169.254.* “it looks like IB” vibes)

Link-local `169.254.0.0/16` is just “no DHCP”; it’s not a magic IB/RDMA indicator. Static IPs are cleaner. ([NVIDIA Developer Forums][5])

Pick a private /24 and assign **one IP per half**.

Example: assume your cable is on **port f1** (adjust names if you’re on f0).

**Node A** (`/etc/netplan/40-cx7.yaml`):

```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    enp1s0f1np1:
      dhcp4: no
      addresses: [192.168.177.11/24]
      mtu: 9000
      optional: true
    enP2p1s0f1np1:
      dhcp4: no
      addresses: [192.168.177.12/24]
      mtu: 9000
      optional: true
```

**Node B**:

```yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    enp1s0f1np1:
      dhcp4: no
      addresses: [192.168.177.21/24]
      mtu: 9000
      optional: true
    enP2p1s0f1np1:
      dhcp4: no
      addresses: [192.168.177.22/24]
      mtu: 9000
      optional: true
```

Apply on each node:

```bash
sudo netplan apply
ip -br a | egrep 'enp1s0f|enP2p1s0f'
```

**Why MTU 9000?** It’s not a miracle, but it helps. In practice, people hit ~196–198 Gb/s aggregate after setting jumbo frames end-to-end. ([NVIDIA Developer Forums][4])

---

### Step C — Disable IPv6 on the CX7 interfaces (practical RoCE stability move)

Multiple folks disable IPv6 on these interfaces to keep **RoCE GID indexing stable**. ([NVIDIA Developer Forums][4])

Quick (runtime) disable:

```bash
sudo sysctl -w net.ipv6.conf.enp1s0f1np1.disable_ipv6=1
sudo sysctl -w net.ipv6.conf.enP2p1s0f1np1.disable_ipv6=1
```

Persist it (create a file):

```bash
sudo tee /etc/sysctl.d/90-spark-cx7-ipv6.conf >/dev/null <<'EOF'
net.ipv6.conf.enp1s0f1np1.disable_ipv6=1
net.ipv6.conf.enP2p1s0f1np1.disable_ipv6=1
EOF

sudo sysctl --system
```

---

### Step D — Verify you can actually push ~200 Gb/s (2× ~100 Gb/s)

**You only get ~200 Gb/s aggregate if you drive both halves concurrently.**

On **Node B**:

```bash
iperf3 -s
```

On **Node A**, run these **at the same time** (two terminals):

Terminal 1 (half 1):

```bash
iperf3 -c 192.168.177.21 -B 192.168.177.11 -P 16 -t 30
```

Terminal 2 (half 2):

```bash
iperf3 -c 192.168.177.22 -B 192.168.177.12 -P 16 -t 30
```

Expected shape:

* each session ~100 Gb/s-ish
* aggregate ~196–198 Gb/s if everything is right ([NVIDIA Developer Forums][4])

If you only run one session and get ~100 Gb/s: **that’s normal on Spark**. ([NVIDIA Developer Forums][1])

---

## 3) Make NCCL actually use RoCE (NET/IB), not sockets

### Critical gotcha: missing RDMA libs inside containers → NCCL silently falls back

If `rdma-core` (verbs libs) aren’t available **inside** your container, NCCL can’t use IB/RDMA and will fall back to Ethernet/socket. This shows up as “Using network Socket” in logs. ([NVIDIA Developer Forums][5])

### The “smoking gun” log lines you want

When it’s working, NCCL logs will look like:

* `NCCL_IB_HCA set to rocep...`
* `NET/IB : Using ... RoCE ... OOB enp...`
* `Initialized NET plugin IB`
* `Using network IB` ([NVIDIA Developer Forums][5])

If instead you see `Using network Socket`, you’re leaving a lot on the table.

### Also check for **/etc/nccl.conf sabotage**

People have found a leftover `/etc/nccl.conf` forcing Socket and/or `NCCL_IB_DISABLE=1`, which breaks NET/IB init. ([NVIDIA Developer Forums][4])

---

## 4) GPUDirect RDMA: don’t chase it on Spark

This is non-negotiable: **GPUDirect RDMA is not supported on DGX Spark** due to its unified memory architecture; mechanisms like `nvidia-peermem`, dma-buf, GDRCopy won’t work. NVIDIA explicitly recommends host-buffer fallbacks (e.g., `cudaHostAlloc` + `ib_reg_mr`). ([nvidia.custhelp.com][6])

So the winning plan is: **RoCE works, but it’s host-staged.** You can still hit ~200 Gb/s class, just don’t expect some magical “turn on GPUDirect and get 400 Gb/s.”

---

## 5) Should you use two cables?

**My conclusion:** No, not for throughput. With Spark’s architecture, even “two links” testing often still tops out around the same ~200 Gb/s class overall. ([NVIDIA Developer Forums][1])

**Popular opinion in the Spark community:** “Don’t bother with dual cables unless you’re doing redundancy or you enjoy experiments.” The real performance unlock is **getting NET/IB working** and driving both halves correctly, not adding more copper. ([NVIDIA Developer Forums][4])

---

## 6) Max batched throughput for Qwen 235B on 2× DGX Spark

If you mean **Qwen3‑235B‑A22B**, NVIDIA’s own Spark playbook basically spells out the “best known” stack:

* **Runtime:** TensorRT‑LLM
* **Quant:** NVFP4
* **Deployment:** “two Sparks only”
* **Model handle:** `nvidia/Qwen3-235B-A22B-FP4` ([NVIDIA NIM APIs][7])

And NVIDIA published a reference datapoint on dual Spark:

* **Prompt processing:** **23,477 tokens/s**
* **Token generation:** **11.73 tokens/s**
  at **ISL 2048 / OSL 128 / BS=1** ([NVIDIA Developer][2])

### The straight “do this” recipe (NVIDIA’s two‑Spark TRT‑LLM workflow)

Below is the core of NVIDIA’s “Run on two Sparks” playbook; this is the most direct path to “max throughput” without you inventing your own cluster scripts. ([NVIDIA NIM APIs][8])

#### 1) Start TRT‑LLM containers on **both nodes**

Run on **each** node:

```bash
docker run -d --rm \
  --name trtllm-multinode \
  --gpus "device=all" \
  --network host \
  --ulimit memlock=-1 \
  --ulimit stack=67108864 \
  --device /dev/infiniband:/dev/infiniband \
  -e UCX_NET_DEVICES="enp1s0f0np0,enp1s0f1np1" \
  -e NCCL_SOCKET_IFNAME="enp1s0f0np0,enp1s0f1np1" \
  -e OMPI_MCA_btl_tcp_if_include="enp1s0f0np0,enp1s0f1np1" \
  -e OMPI_MCA_orte_default_hostfile="/etc/openmpi-hostfile" \
  -e OMPI_MCA_rmaps_ppr_n_pernode="1" \
  -e OMPI_ALLOW_RUN_AS_ROOT="1" \
  -e OMPI_ALLOW_RUN_AS_ROOT_CONFIRM="1" \
  -v ~/.cache/huggingface/:/root/.cache/huggingface/ \
  -v ~/.ssh:/tmp/.ssh:ro \
  nvcr.io/nvidia/tensorrt-llm/release:1.2.0rc6 \
  sh -c "curl https://raw.githubusercontent.com/NVIDIA/dgx-spark-playbooks/refs/heads/main/nvidia/trt-llm/assets/trtllm-mn-entrypoint.sh | sh"
```

Notes:

* If you only connected **one** QSFP port, you may see a UCX warning about the other port missing; the playbook explicitly says you can ignore it if inference works. ([NVIDIA NIM APIs][8])
* If you want to be stricter, set those env vars to only the **active** interface(s) to avoid warnings — but don’t do that until you’ve got a working baseline.

#### 2) Create an MPI hostfile (on your “primary” node)

```bash
cat > ~/openmpi-hostfile <<EOF
192.168.177.11
192.168.177.21
EOF
```

Then:

```bash
docker cp ~/openmpi-hostfile trtllm-multinode:/etc/openmpi-hostfile
export TRTLLM_MN_CONTAINER=trtllm-multinode
```

#### 3) Tell TRT‑LLM to reserve memory for KV cache

(This matters for batch throughput.)

```bash
docker exec $TRTLLM_MN_CONTAINER bash -c 'cat <<EOF > /tmp/extra-llm-api-config.yml
print_iter_log: false
kv_cache_config:
  dtype: "auto"
  free_gpu_memory_fraction: 0.9
cuda_graph_config:
  enable_padding: true
EOF'
```

This matches NVIDIA’s recommended pattern for multinode serving. ([NVIDIA NIM APIs][8])

#### 4) Download + serve Qwen3‑235B‑A22B‑FP4 with tensor parallel = 2

```bash
export HF_TOKEN=<your-huggingface-token>

docker exec \
  -e MODEL="nvidia/Qwen3-235B-A22B-FP4" \
  -e HF_TOKEN=$HF_TOKEN \
  -it $TRTLLM_MN_CONTAINER bash -c 'mpirun -x HF_TOKEN bash -c "hf download $MODEL"'
```

Serve:

```bash
docker exec \
  -e MODEL="nvidia/Qwen3-235B-A22B-FP4" \
  -e HF_TOKEN=$HF_TOKEN \
  -it $TRTLLM_MN_CONTAINER bash -c '
    mpirun -x HF_TOKEN trtllm-llmapi-launch trtllm-serve $MODEL \
      --tp_size 2 \
      --backend pytorch \
      --max_num_tokens 32768 \
      --max_batch_size 4 \
      --extra_llm_api_options /tmp/extra-llm-api-config.yml \
      --port 8355'
```

This is literally the “two Sparks” reference workflow. ([NVIDIA NIM APIs][8])

---

## 7) How to actually push “max batched throughput” beyond the baseline

Here’s the non-hand-wavy tuning logic:

### A) If you want max **prefill throughput**

* Increase **`--max_batch_size`** until you hit memory limits or latency goals.
* Keep **NVFP4** (it’s why this is even possible on Spark). NVFP4 is positioned as a major memory reduction vs FP16/FP8 and typically small accuracy loss. ([NVIDIA NIM APIs][9])
* Make sure you’re not on NET/Socket (RoCE matters most under load).

### B) If you want max **decode throughput**

Decode is the painful part for huge models. The two levers that actually move it:

1. **Speculative decoding**
   NVIDIA has a Spark playbook for it and cites up to **2.6×** improvement vs FP8 for Qwen‑235B on dual Spark (their blog). ([NVIDIA Developer][3])

2. **KV cache precision and headroom**
   Your `free_gpu_memory_fraction: 0.9` and `kv_cache_config` are exactly about this. If you starve KV, your “batched throughput” collapses under concurrency. ([NVIDIA NIM APIs][8])

### C) Use published numbers as your sanity check

If you’re wildly below NVIDIA’s published **11.73 tok/s decode** (BS=1) or your prompt throughput is nowhere near **23k tok/s**, you’re likely:

* not running NVFP4 TRT‑LLM,
* not using RoCE/IB,
* or you’re memory-thrashing (KV). ([NVIDIA Developer][2])

---

## 8) Quick “if X then Y” debugging map

* **You see ~100 Gb/s and can’t exceed it**
  → You’re probably only driving one “half” (or you’re using TCP/Socket). Run **two iperf sessions bound to the two IPs** and check MTU 9000. ([NVIDIA Developer Forums][4])

* **NCCL logs show `Using network Socket`**
  → IB/RDMA path isn’t active. Check container has RDMA libs + /dev/infiniband is mounted; check no `/etc/nccl.conf` is disabling IB. ([NVIDIA Developer Forums][5])

* **You’re trying to “enable GPUDirect RDMA”**
  → Stop. It’s not supported on Spark. Use host-staged RDMA; that’s the intended fallback. ([nvidia.custhelp.com][6])

---

If you want, paste **(a)** your `ibdev2netdev` output and **(b)** one NCCL log snippet showing whether it says `NET/IB` or `NET/Socket`, and I’ll tell you exactly which layer is still wrong — but the steps above are the shortest path to “single cable, ~200 Gb/s class, Qwen‑235B max throughput.”

[1]: https://forums.developer.nvidia.com/t/connectx-7-nic-in-dgx-spark/350417?page=4 "ConnectX-7 NIC in DGX Spark - Page 4 - DGX Spark / GB10 - NVIDIA Developer Forums"
[2]: https://developer.nvidia.com/blog/how-nvidia-dgx-sparks-performance-enables-intensive-ai-tasks/ "How NVIDIA DGX Spark’s Performance Enables Intensive AI Tasks | NVIDIA Technical Blog"
[3]: https://developer.nvidia.com/blog/new-software-and-model-optimizations-supercharge-nvidia-dgx-spark/ "New Software and Model Optimizations Supercharge NVIDIA DGX Spark | NVIDIA Technical Blog"
[4]: https://forums.developer.nvidia.com/t/dgx-spark-gb10-connectx-7-200gbe-via-mikrotik-crs812-qsfp-dd-2xqsfp56-breakout/357162 "https://forums.developer.nvidia.com/t/dgx-spark-gb10-connectx-7-200gbe-via-mikrotik-crs812-qsfp-dd-2xqsfp56-breakout/357162"
[5]: https://forums.developer.nvidia.com/t/install-and-use-vllm-for-inference-on-two-sparks-does-not-work/349496?page=5 "Install and Use vLLM for Inference on two Sparks does not work - Page 5 - DGX Spark / GB10 - NVIDIA Developer Forums"
[6]: https://nvidia.custhelp.com/app/answers/detail/a_id/5780/~/is-gpudirect-rdma-supported-on-dgx-spark%3F "https://nvidia.custhelp.com/app/answers/detail/a_id/5780/~/is-gpudirect-rdma-supported-on-dgx-spark%3F"
[7]: https://build.nvidia.com/spark/trt-llm "https://build.nvidia.com/spark/trt-llm"
[8]: https://build.nvidia.com/spark/trt-llm/stacked-sparks "https://build.nvidia.com/spark/trt-llm/stacked-sparks"
[9]: https://build.nvidia.com/spark/nvfp4-quantization "NVFP4 Quantization | DGX Spark"

  **TLDR**

* ✅ **“One QSFP cable can reach full 200GbE” is explicitly NVIDIA’s guidance** (it’s written in NVIDIA’s Connect Two Sparks + NCCL playbooks). ([build.nvidia.com][1])
* ✅ **“GPUDirect RDMA is not supported on DGX Spark/GB10” is explicitly stated by NVIDIA** (and they explain why). ([NVIDIA Developer Forums][2])
* ✅ **“InfiniBand *mode* isn’t supported” is basically correct**: NVIDIA’s Spark user guide says the CX-7 ports support **Ethernet configuration only** (i.e., RoCE not IB link-layer). ([NVIDIA Docs][3])
* ❌ **“single 100G interface” is wrong/misleading**: people see **200000Mb/s** link speed on a single cable, and NVIDIA playbooks call it **200GbE**. ([NVIDIA Developer Forums][4])
* ⚠️ **“Socket transport is the official method” is overstated**: NVIDIA playbooks set `NCCL_SOCKET_IFNAME`, but NCCL still shows **NET/IB** (ibverbs/RoCE) with **GDR disabled** in real runs—so “socket-only” is not what’s actually happening by default. ([build.nvidia.com][5])
* ❓ **“~9 Gbit/s is normal for TP=2 traffic” is not supported by the cited source** and is not something NVIDIA states; it might be a *workload-specific observation*, but you can’t generalize it.

---

## Claim-by-claim fact check

### 1) “Current Setup is NVIDIA Official”

**Verdict: depends on what “current setup” is.**

If “current setup” means:

* connect 2 Sparks with QSFP/CX7,
* configure the high-speed interface,
* and run NCCL/vLLM using the playbooks,

…then yes, that workflow is official and documented. ([build.nvidia.com][6])

If it means “assume only 100G is possible” or “assume sockets are the only real transport,” then **no**—that’s not what NVIDIA documents or what NCCL logs show in practice. ([NVIDIA Developer Forums][7])

---

### 2) “NVIDIA recommends single 100G interface + socket transport for dual Spark.”

**Verdict: mostly wrong / muddled.**

What NVIDIA actually documents:

* They describe the direct interconnect as **200GbE** with a QSFP cable. ([build.nvidia.com][1])
* They explicitly say **one QSFP cable is enough for full bandwidth**. ([build.nvidia.com][1])
* They do set `NCCL_SOCKET_IFNAME` and other TCP/socket-ish vars in both the NCCL and vLLM playbooks. ([build.nvidia.com][5])

What’s wrong in the plan:

* **“100G”**: Users report **ethtool Speed: 200000Mb/s** on a single back-to-back cable. ([NVIDIA Developer Forums][4])
* **“socket transport”**: `NCCL_SOCKET_IFNAME` is present in the playbook, but real NCCL runs still log **NET/IB** (RoCE/ibverbs) with GDR disabled; that’s not “socket-only.” ([NVIDIA Developer Forums][7])

So: **single-cable is right; “100G + socket transport” is not.**

---

### 3) “IB mode NOT supported (no GPUDirect on GB10)”

**Verdict: partially true, but needs precise wording.**

* **InfiniBand link-layer / IB mode:** NVIDIA’s DGX Spark user guide states: **“The DGX Spark CX-7 ports support ethernet configuration only.”** ([NVIDIA Docs][3])
  That supports “IB mode not supported” *if* you mean “cannot be configured as InfiniBand link layer.”

* **GPUDirect RDMA:** NVIDIA explicitly says **GPUDirect RDMA is not supported**, and mechanisms like `nvidia-peermem`, dma-buf, GDRCopy “do not work.” ([NVIDIA Developer Forums][2])

Important nuance you should not lose:

* Ethernet-only **does not mean “no RDMA.”** It means RoCE (RDMA over Ethernet) is the path. NCCL even logs `NET/IB` and “GDR disabled” against `roce*` HCAs. ([NVIDIA Developer Forums][7])

So the accurate statement is:

> **Ethernet-only (RoCE), and GPUDirect RDMA is unsupported** (expect CPU-bounce behavior).

---

### 4) “One QSFP cable = full 200 Gbps (dual not required)”

**Verdict: true (official + corroborated).**

NVIDIA’s playbooks literally say:

* **“Full bandwidth can be achieved with just one QSFP cable.”** ([build.nvidia.com][1])

And forum users report the port negotiating at **200000Mb/s** using a single cable. ([NVIDIA Developer Forums][4])

Also: there’s a common pitfall where plugging *both* cables without configuring all interfaces can effectively drop throughput. NVIDIA forum guidance indicates connecting both ports can “bump down speed to 100Gbps” in certain setups. ([NVIDIA Developer Forums][7])

---

### 5) “Socket transport is official method”

**Verdict: misleading / not demonstrated by the sources.**

What’s true:

* NVIDIA playbooks set `NCCL_SOCKET_IFNAME` and other TCP interface selectors. ([build.nvidia.com][5])

What that does **not** prove:

* That NCCL is using “socket transport” as the primary data path.

Evidence against “socket-only”:

* In NCCL debug output from DGX Spark users, NCCL reports **`NET/IB`** and explicitly says **“GPU Direct RDMA Disabled for HCA … ‘roce…’”**. That means NCCL is going through its IB/verbs network path (RoCE/ibverbs), but without GPUDirect RDMA. ([NVIDIA Developer Forums][7])

So a better phrasing is:

* **Official playbooks configure the socket interface selection**, but NCCL may still use **RoCE/ibverbs** (logged as NET/IB) with GDR disabled.

---

### 6) “~9 Gbit/s is normal for TP=2 traffic”

**Verdict: not verifiable from the cited source; likely misleading as a blanket statement.**

* The cited NVIDIA vLLM playbook **does not** say anything like “expect ~9 Gbit/s.” ([build.nvidia.com][6])
* We *do* have evidence that the interconnect can do roughly:

  * ~**22 GB/s** “busbw” in `all_gather_perf` when configured right (that’s ~176 Gbps; 25 GB/s would be 200 Gbps theoretical). ([NVIDIA Developer Forums][7])
  * and users mention **24–25 GB/s** expectations in context of “200Gbps.” ([NVIDIA Developer Forums][4])

If someone saw **9 Gbit/s** during TP=2 inference, it could be:

* a measurement artifact,
* a workload that doesn’t saturate bandwidth,
* CPU-bound behavior,
* or simply “bits vs bytes” confusion (9 **GB/s** vs 9 **Gb/s** is an 8× difference).

But you cannot call it “normal” without a defined workload + measurement + baseline.

---

## The biggest issue: the stated “Source” doesn’t support several bullets

Your plan says the source is `build.nvidia.com/spark/vllm/stacked-sparks`.

That page **does** show the socket interface env vars and points to the connectivity playbook. ([build.nvidia.com][6])
But it **does not** itself establish:

* Ethernet-only ports,
* GPUDirect RDMA unsupported,
* or any “9 Gbit/s is normal” performance claim.

Those come from:

* DGX Spark User Guide “Spark Stacking” (Ethernet-only) ([NVIDIA Docs][3])
* NVIDIA forum FAQ / moderator answers (GPUDirect RDMA unsupported) ([NVIDIA Developer Forums][2])
* NVIDIA playbooks + forum measurements (one-cable full bandwidth + link speed). ([build.nvidia.com][1])

---

## If you want a corrected “CLAUDE.md guidance” snippet (drop-in)

Here’s a clean, accurate version you can paste:

* **Interconnect:** DGX Spark stacking uses **CX-7 QSFP ports in Ethernet-only configuration (RoCE)**, not InfiniBand link-layer. ([NVIDIA Docs][3])
* **Cabling:** **One QSFP cable is sufficient for full 200GbE bandwidth**; two cables require IP configuration across all interfaces for full aggregate bandwidth. ([build.nvidia.com][1])
* **GPUDirect:** **GPUDirect RDMA is not supported** on DGX Spark/GB10; expect CPU bounce-buffer behavior (NCCL logs show GDR disabled). ([NVIDIA Developer Forums][2])
* **Validation:** Use `nccl-tests` (`all_gather_perf`) and/or `ethtool` to confirm you’re near expected throughput (≈22–24 GB/s is typical for a 200Gbps link with overhead). ([NVIDIA Developer Forums][7])

If you paste your actual “current setup” (what cables, which ports, what env vars, what benchmark output), I can sanity-check whether it matches the official playbooks line-by-line.

[1]: https://build.nvidia.com/spark/connect-two-sparks/stacked-sparks "Connect Two Sparks | DGX Spark"
[2]: https://forums.developer.nvidia.com/t/dgx-spark-gb10-faq/347344 "DGX Spark / GB10 FAQ - DGX Spark / GB10 - NVIDIA Developer Forums"
[3]: https://docs.nvidia.com/dgx/dgx-spark/spark-clustering.html "Spark Stacking — DGX Spark User Guide"
[4]: https://forums.developer.nvidia.com/t/confusion-surrounding-the-qsfp-ports-and-bandwidth/356092 "Confusion surrounding the QSFP ports and bandwidth - DGX Spark / GB10 - NVIDIA Developer Forums"
[5]: https://build.nvidia.com/spark/nccl/stacked-sparks "NCCL for Two Sparks | DGX Spark"
[6]: https://build.nvidia.com/spark/vllm/stacked-sparks "vLLM for Inference | DGX Spark"
[7]: https://forums.developer.nvidia.com/t/dgx-spark-nccl-test-10gb-s-not-200-gbps-25-gb-s/350077 "DGX Spark NCCL Test: 10GB/s not 200 Gbps=25 GB/s - DGX Spark / GB10 - NVIDIA Developer Forums"

**TLDR**

* If you’re not explicitly passing `--mm-encoder-tp-mode data`, then **no** — you’re **not** doing “data-parallel mm encoding”; vLLM defaults to `weights`. ([vLLM][1])
* On a **dual DGX Spark** setup (TP=2 across two systems), you **probably should** enable `--mm-encoder-tp-mode data` **for vision-language models that support it**, because it avoids the vision-encoder per-layer comms overhead that can be brutal across nodes. vLLM’s own recipes recommend it for exactly that reason. ([vLLM][2])
* The two real caveats: **(1)** slightly higher memory (encoder weights replicated), **(2)** it can interact badly with **multimodal profiling/startup** in some setups—use `--skip-mm-profiling` + leave memory headroom if that’s what bit you. ([vLLM][1])

---

## 1) What “data-parallel mm encoding” actually means in vLLM

`--mm-encoder-tp-mode` only affects the **multimodal encoder** (typically a ViT). It has two modes: ([vLLM][1])

* **`weights` (default):** encoder weights are *sharded* across TP ranks (classic tensor parallel).
* **`data`:** encoder weights are *replicated* on every TP rank, and the **image batch is split across ranks** (“batch-level DP”). Importantly: **this is not the same thing as** request-level `--data-parallel-size`. ([vLLM][1])

So yes, the name is confusing: it’s “DP-like” but **inside one engine**, using the same TP ranks.

---

## 2) Are you doing it right now?

You are **only** doing it if your launch includes:

```bash
--mm-encoder-tp-mode data
```

If you don’t specify it, vLLM’s default is:

* `--mm-encoder-tp-mode weights` ([vLLM][1])

Also: even if you pass `data`, vLLM can silently **fall back to `weights`** for models that don’t implement it, and logs a warning. ([vLLM][3])

---

## 3) Should you enable it on dual DGX Spark?

### My conclusion (tell-it-like-it-is)

**Yes — if you are serving image/video inputs and your model supports it, `--mm-encoder-tp-mode data` is usually the right move** on a 2-node Spark cluster.

Reason: on Spark you’re explicitly using the **ConnectX networking to span nodes** (NVIDIA even markets that 2-unit setup for scaling up to ~405B-param models). That’s great for making big models fit, but it also means **cross-node collectives are a constant tax.** ([NVIDIA][4])

Batch-level DP for the vision encoder is designed to remove the **per-layer synchronization** overhead from the vision forward pass, which is often “all pain, little gain” because the vision encoder is small relative to the decoder. vLLM’s own Qwen VL recipes explicitly recommend `data` mode for this exact reason (small encoder → TP adds comm overhead; `data` can be faster; costs some memory). ([vLLM][2])

### Popular opinion (what most people do)

* Among people actively serving VL models in vLLM: **enable it** when supported (especially at higher TP), because it’s “one flag, real win.” ([vLLM][2])
* Conservative ops folks: leave default `weights` unless they’ve tested stability, because multimodal stacks can be finicky and bugs happen.

### Concrete evidence it’s used on dual Spark in the wild

On NVIDIA’s DGX Spark forum, people running **TP=2 across a dual Spark cluster** are explicitly using `--mm-encoder-tp-mode data` (often with lower `--gpu-memory-utilization`, like `0.7`) and reporting usable performance. ([NVIDIA Developer Forums][5])

---

## 4) Why it helps (and what you give up)

### Why it tends to help

* Vision encoders are typically a tiny fraction of total params; sharding them buys little, but forces synchronization.

  * Example from vLLM docs: in Qwen2.5-VL-72B the ViT is **675M params vs 72B** LM params — i.e., the encoder is small, so TP on it is mostly comm overhead. ([vLLM][6])
* Benchmarks (different hardware than yours, but shows the effect size): switching to DP-mode for the vision encoder improved request throughput anywhere from basically flat to **~+44%** depending on image size/items per request in one study. ([rocm.blogs.amd.com][7])

### What you give up / risks

1. **More memory**: encoder weights replicate on every TP rank (per GPU memory goes up a bit). vLLM recipes explicitly warn you may need to tune `--gpu-memory-utilization`. ([vLLM][2])
2. **More “moving parts”** in distributed execution. If you already saw **multi-node hangs during multimodal profiling**, don’t ignore that.

---

## 5) The hang you mentioned: the pragmatic workaround

If the hang is happening during vLLM’s **multimodal memory profiling / init**, the most direct lever is:

* `--skip-mm-profiling`

vLLM documents that this **skips multimodal memory profiling**, reduces startup time, and **pushes the responsibility onto you** to leave enough memory headroom for encoder activations/embedding cache. ([vLLM][1])

In practice on dual Spark, people often pair “harder” configs with a conservative memory utilization (example: `--gpu-memory-utilization 0.7`). ([NVIDIA Developer Forums][5])

If you go this route, do it deliberately:

* lower `--gpu-memory-utilization` first
* cap multimodal payloads via `--limit-mm-per-prompt` (images/videos) to prevent someone from sending a monster request and detonating your memory ([vLLM][6])

---

## 6) What I’d do on dual DGX Spark

### If you actually serve images (VL)

1. Enable:

   * `--mm-encoder-tp-mode data` ([vLLM][1])
2. If you hit startup hangs:

   * add `--skip-mm-profiling`
   * drop `--gpu-memory-utilization` (start conservative, then raise) ([vLLM][2])
3. Put guardrails:

   * `--limit-mm-per-prompt '{"image":<N>,"video":0}'` (or similar) ([vLLM][6])

### If you’re mostly text-only

Don’t overthink it: **disable multimodal entirely** (so there’s no encoder work to optimize). vLLM’s Qwen3-VL guide explicitly calls out that setting `--limit-mm-per-prompt.image 0 --limit-mm-per-prompt.video 0` skips the vision encoder + multimodal profiling and frees memory for KV cache. ([vLLM][2])

---

## 7) One more “don’t get burned” note

Long-running multimodal serving can still hit weird GPU memory behavior. There are open bug reports of VRAM usage growing over days with VL workloads (not necessarily caused by `mm-encoder-tp-mode data`, but it’s a real ops risk). Watch memory, and be ready to roll restarts if needed. ([GitHub][8])

[1]: https://docs.vllm.ai/en/v0.10.2/configuration/engine_args.html "https://docs.vllm.ai/en/v0.10.2/configuration/engine_args.html"
[2]: https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html "https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html"
[3]: https://docs.vllm.ai/en/v0.10.2/api/vllm/config/index.html "https://docs.vllm.ai/en/v0.10.2/api/vllm/config/index.html"
[4]: https://www.nvidia.com/en-us/products/workstations/dgx-spark/ "https://www.nvidia.com/en-us/products/workstations/dgx-spark/"
[5]: https://forums.developer.nvidia.com/t/dgx-spark-nemotron3-and-nvfp4-getting-to-65-tps/355261 "https://forums.developer.nvidia.com/t/dgx-spark-nemotron3-and-nvfp4-getting-to-65-tps/355261"
[6]: https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen2.5-VL.html "https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen2.5-VL.html"
[7]: https://rocm.blogs.amd.com/software-tools-optimization/vllm-dp-vision/README.html "https://rocm.blogs.amd.com/software-tools-optimization/vllm-dp-vision/README.html"
[8]: https://github.com/vllm-project/vllm/issues/28230 "https://github.com/vllm-project/vllm/issues/28230"

**TLDR**

* **Pick option 1 (Enable RoCE / `NCCL_NET=IB`)**. Dual DGX Spark = **two-node, two-GPU** setup, and you only get sane multi-node perf if you use the ConnectX RDMA path. DGX Spark’s CX-7 ports are **Ethernet-only**, so this is **RoCE**, not “real InfiniBand.” ([NVIDIA Docs][1])
* For **Qwen3‑VL‑235B‑A22B @ 4‑bit**, start with **vLLM + AWQ 4‑bit** (least pain, already proven by multiple Spark users). A real datapoint: **~26 tok/s @ concurrency=1** and **~63 tok/s @ concurrency=10** on a **2‑node Spark cluster** for the AWQ model. ([NVIDIA Developer Forums][2])
* **TRT‑LLM is not the first choice for *Qwen3‑VL‑235B*** today because NVIDIA’s Spark TRT‑LLM “supported model” list explicitly includes **Qwen3‑235B (text-only)** on two Sparks, but **doesn’t list Qwen3‑VL‑235B**—so you’re in “DIY / sharp edges” territory. ([NVIDIA NIM APIs][3])

---

## What “right choice” means here

You’re trying to run **a 235B MoE VLM** on **two separate machines**. That breaks into two hard requirements:

1. **Make two Sparks behave like one distributed inference box.**
   That’s *network + NCCL + RDMA*. (This is option **1**.)

2. **Pick an inference stack that actually runs Qwen3‑VL‑235B in 4-bit without you spelunking kernel/plugin hell.**
   That’s usually **vLLM + AWQ** right now for this specific model.

So if your UI forces a *single* choice: **choose 1**. Then choose your backend afterward.

---

## Why option 1 is basically mandatory on dual DGX Spark

* NVIDIA positions DGX Spark’s **ConnectX networking** specifically to “connect two DGX Spark systems” and run models up to **~405B parameters**. ([NVIDIA][4])
* The official Spark “stacking / clustering” guidance is literally about wiring two Sparks via the CX-7 and using **MPI + NCCL**—and it explicitly notes the CX-7 ports are **Ethernet configuration only** (so: RoCE, not IB fabric). ([NVIDIA Docs][1])

**Reality check (don’t assume marketing numbers):**
A reviewer found the ConnectX-7 sits behind a **PCIe Gen5 x4** link and measured ~**96 Gb/s** even when pushing traffic across both ports in iperf, implying you may not actually realize a full 200 Gb/s of usable throughput. ([ServeTheHome][5])
That doesn’t make RoCE useless—it just means **your scaling ceiling may be lower than “200G” implies**, so you should optimize for **low latency + stable RDMA**, not chase a perfect headline number.

---

## Backend choice for Qwen3‑VL‑235B @ 4-bit on two Sparks

### The pragmatic “it works” choice: **vLLM + AWQ 4-bit**

Evidence from Spark users is strong enough that I’d treat this as the default:

* In NVIDIA’s own DGX Spark forum benchmarks, **QuantTrio/Qwen3‑VL‑235B‑A22B‑Instruct‑AWQ** on a **2‑node** configuration is reported at:

  * **TPS=26** at **concurrency=1**
  * **TPS=63** at **concurrency=10** ([NVIDIA Developer Forums][2])
* Same thread: AWQ is reported to **materially reduce latency** vs FP8 for this model (example numbers in that report: **TPOT 44ms → 30ms**, **TTFT 128ms → 88ms** at low concurrency). ([NVIDIA Developer Forums][6])
* The AWQ checkpoint exists and is widely used (e.g., QuantTrio). ([Hugging Face][7])

**My conclusion:** for “Qwen3‑VL‑235B 4‑bit on 2× Spark”, **vLLM + AWQ** is the best first move.

**Popular opinion (what you’ll see in practice):** same—most people doing this on Sparks start with vLLM/SGLang + AWQ, because it’s the shortest path to working throughput without engine-building drama.

---

### The “4-bit but FP4” option: **NVFP4 checkpoints (still usually vLLM first)**

If by “4bit” you mean **FP4/NVFP4** (not int4 weight-only), there are real checkpoints:

* Example: **RedHatAI/Qwen3‑VL‑235B‑A22B‑Instruct‑NVFP4** says it’s FP4 (weights+activations) and “ready for inference with vLLM >= 0.9.1”, claiming ~75% memory reduction vs 16-bit. ([Hugging Face][8])

**But be skeptical:** NVFP4 on MoE + new hardware paths has had churn across stacks (and you may hit “supported in theory” vs “fast and stable in practice”). So I’d treat NVFP4 as **phase 2**, after AWQ is running.

---

### Why I *wouldn’t* start with TRT‑LLM for Qwen3‑VL‑235B (today)

NVIDIA’s DGX Spark TRT‑LLM playbook has a **support matrix**. It includes **Qwen3‑235B-A22B (two Sparks only)** in NVFP4… but **not Qwen3‑VL‑235B**. ([NVIDIA NIM APIs][3])

So if you “Switch to TRT‑LLM” right away, you’re likely signing up for:

* custom conversion/quantization steps,
* potential missing kernels/plugins for your exact VL MoE path,
* extra debugging you didn’t need.

TRT‑LLM *is* the long-term bet for peak throughput, but for *this specific model*, on *this specific dual-node desktop-ish cluster*, the support signal says: **not the shortest path**.

---

## A sane decision tree

### If your goal is “run Qwen3‑VL‑235B 4-bit successfully this week”

1. **Option 1: RoCE/`NCCL_NET=IB`** (cluster plumbing) ([NVIDIA Docs][1])
2. **vLLM + AWQ 4-bit model** (QuantTrio AWQ is a known-good direction) ([NVIDIA Developer Forums][2])

### If your goal is “squeeze max performance later”

1. Still **Option 1** first
2. Try **NVFP4** checkpoint(s) once stable
3. Only then evaluate **TRT‑LLM** when Qwen3‑VL appears in a supported Spark recipe/matrix (or you’re ready to DIY).

---

## Tiny but important gotchas

* Don’t waste time trying to “flip the port into InfiniBand mode.” DGX Spark CX-7 ports are documented as **Ethernet-only**—you’re doing **RoCE**. ([NVIDIA Docs][1])
* Expect **bandwidth constraints** vs server-class multi-GPU boxes; one published datapoint suggests ~**96 Gb/s** effective in simple tests, which can cap distributed scaling if your workload becomes comm-heavy. ([ServeTheHome][5])
* The numbers that look insane (like **23,477 tok/s**) are **prefill throughput** for text-only Qwen3‑235B in NVIDIA’s marketing/bench context, not the same as steady-state VLM generation in your app. ([NVIDIA Developer][9])

---

## Bottom line

For **Qwen3‑VL‑235B @ 4-bit on dual DGX Spark**, the “right choice” is:

**✅ Option 1 (Enable RoCE / `NCCL_NET=IB`) + vLLM with an AWQ 4-bit checkpoint**, because it’s the most evidence-backed path with real reported throughput on a 2-node Spark cluster (~**26 tok/s** at concurrency 1). ([NVIDIA Developer Forums][2])

If you tell me your target serving mode (single-user interactive vs multi-user throughput, and your context length target like 8k vs 128k vs 256k), I can give you a sharper “expected TPS + what to tune first” plan.

[1]: https://docs.nvidia.com/dgx/dgx-spark/spark-clustering.html "Spark Stacking — DGX Spark User Guide"
[2]: https://forums.developer.nvidia.com/t/6x-spark-setup/354399?page=2 "https://forums.developer.nvidia.com/t/6x-spark-setup/354399?page=2"
[3]: https://build.nvidia.com/spark/trt-llm "TRT LLM for Inference | DGX Spark"
[4]: https://www.nvidia.com/en-us/products/workstations/dgx-spark/ "A Grace Blackwell AI supercomputer on your desk | NVIDIA DGX Spark"
[5]: https://www.servethehome.com/nvidia-dgx-spark-review-the-gb10-machine-is-so-freaking-cool/2/ "NVIDIA DGX Spark Review The GB10 Machine is so Freaking Cool - Page 2 of 4 - ServeTheHome"
[6]: https://forums.developer.nvidia.com/t/6x-spark-setup/354399 "https://forums.developer.nvidia.com/t/6x-spark-setup/354399"
[7]: https://huggingface.co/QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ "https://huggingface.co/QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ"
[8]: https://huggingface.co/RedHatAI/Qwen3-VL-235B-A22B-Instruct-NVFP4?utm_source=chatgpt.com "RedHatAI/Qwen3-VL-235B-A22B-Instruct-NVFP4"
[9]: https://developer.nvidia.com/blog/how-nvidia-dgx-sparks-performance-enables-intensive-ai-tasks/ "https://developer.nvidia.com/blog/how-nvidia-dgx-sparks-performance-enables-intensive-ai-tasks/"

**TLDR**

* You’re right about the *hardware reality*: DGX Spark / **GB10 does not support GPUDirect RDMA**, and NVIDIA explicitly says **dma-buf / nvidia‑peermem / GDRCopy don’t work** on this platform. ([NVIDIA Developer Forums][1])
* But your “**you’re stuck with Socket (~100 Gb/s) because NCCL requires GPUDirect for IB**” conclusion is **too strong**. There are confirmed Spark runs where **NCCL uses NET/IB with “GPU Direct RDMA Disabled” and still hits ~22 GB/s busbw (~176 Gb/s)**. ([NVIDIA Developer Forums][2])
* What *actually* seems broken is **your specific NCCL inside the NGC container** (very likely an external net plugin / IBext path) enabling **DMABUF anyway** and not honoring the “disable” knobs, which matches known NCCL/plugin weirdness. ([GitHub][3])
* Your workaround **`NCCL_NET=Socket` is valid and documented**. ([NVIDIA Docs][4])
* If you want a serious shot at **~190–200 Gb/s inside NCCL**, the more plausible fix is: **use the Spark playbook NCCL build (v2.28.9-1)** or force NCCL off external plugins + off dmabuf (details below). ([build.nvidia.com][5])

---

## 1) The part you nailed: “GB10 can’t do GPUDirect RDMA” is not opinion, it’s NVIDIA’s statement

NVIDIA’s own DGX Spark / GB10 forum response is unambiguous:

* On DGX Spark’s unified-memory architecture, **memory from `cudaMalloc` cannot be coherently accessed by CPU or PCIe devices**, so **GPUDirect RDMA is not supported**, and **direct I/O mechanisms (nvidia‑peermem, dma‑buf, GDRCopy) don’t work**. ([NVIDIA Developer Forums][1])
* They explicitly recommend that **applications should introspect** (CUDA device attributes) and **fall back**, e.g. allocate comm buffers with `cudaHostAlloc` and register them with `ib_reg_mr`. ([NVIDIA Developer Forums][1])

So the “RDMA bandwidth exists (host ↔ host) but GPU-direct doesn’t” framing is correct.

---

## 2) Where your writeup overreaches: “NCCL IB requires GPUDirect” is not generally true

Two hard counterpoints from official-ish sources:

1. **NCCL itself supports running IB without GPU Direct RDMA.**
   You can explicitly disable GDR with `NCCL_NET_GDR_LEVEL=LOC` (or legacy integer 0), and there are further knobs like `NCCL_NET_GDR_READ`. ([NVIDIA Docs][4])

2. On DGX Spark specifically, people have run NCCL tests showing:

   * `NET/IB : GPU Direct RDMA Disabled ...`
   * Yet they still get **~22 GB/s bus bandwidth** on a large `all_gather_perf` run (≈ **176 Gb/s**, close to a 200G link’s practical ceiling). ([NVIDIA Developer Forums][2])

So the truthful statement is closer to:

> “In *our NGC container stack*, NCCL’s IB path tries DMABUF and crashes instead of staging.”

That’s a *stack-specific failure mode*, not a fundamental NCCL requirement.

---

## 3) The most likely root cause: you’re hitting an external net plugin / dmabuf path that doesn’t degrade gracefully

The log line you quoted:

> `NET/IB : GPU Direct RDMA (DMABUF) enabled ...` then crash

…screams “**dma-buf registration path got selected**.” NCCL has an explicit switch for that:

* `NCCL_DMABUF_ENABLE` (since 2.13) controls “GPU Direct RDMA buffer registration using the Linux dma-buf subsystem.” ([NVIDIA Docs][4])

And you’re also probably dealing with a **net plugin** (common in NGC/HPC stacks). NCCL will load `libnccl-net.so` (or variants) unless told otherwise; you can disable external net plugins by setting:

* `NCCL_NET_PLUGIN=none` ([NVIDIA Docs][4])

This matters because:

* **`NCCL_IB_DISABLE=1` is *supposed* to force sockets**, but there are known cases where it’s ignored when IBext/net plugins get involved. ([NVIDIA Docs][6])
* “Removing `/dev/infiniband`” doesn’t stop NCCL from *loading a plugin* and trying to do IB-ish init anyway; it just makes it fail in weirder ways.

So your observation (“no graceful fallback”) is believable **for that container/plugin combo**, but the **fix space is bigger than `NCCL_NET=Socket`**.

---

## 4) Your workaround is real and correct, but it’s the “safe mode” option

Per NCCL docs:

* `NCCL_NET` can force a specific network module; internal names include **“IB”** and **“Socket”**. ([NVIDIA Docs][4])

So:

```bash
export NCCL_NET=Socket
```

is a legit way to bypass IB entirely.

If your workload only used ~**2 Gb/s** during inference, that’s:

* ~0.25 GB/s
* ~**2% of a 100 Gb/s link**
* ~**1% of a 200 Gb/s link**
  So for TP=2 inference, your conclusion “it’s enough” is numerically solid.

---

## 5) If you want ~200 Gb/s *in NCCL* instead of ib_write_bw, try these two paths (in order)

### Path A: Use the official Spark NCCL build path (most promising)

NVIDIA’s own “NCCL for Two Sparks” playbook tells you to build NCCL **v2.28.9-1** with Blackwell support and then run tests. ([build.nvidia.com][5])

It also explicitly notes:

* **One QSFP cable can hit full bandwidth**
* With **two QSFP cables**, you must assign IPs to **all four** interfaces to get full bandwidth ([build.nvidia.com][5])

If you’re inside an NGC container that bundles a different NCCL / plugin stack, this is the cleanest way to “get onto the known-good path.”

### Path B: Stay in your container, but disable the “bad” pieces (plugin + dmabuf + C2C override)

Try forcing NCCL to avoid the exact mechanism that’s killing you:

```bash
# 1) Disable external net plugins (common culprit in containers)
export NCCL_NET_PLUGIN=none

# 2) Disable dma-buf registration explicitly
export NCCL_DMABUF_ENABLE=0

# 3) Hard-disable GPU Direct RDMA
export NCCL_NET_GDR_LEVEL=LOC

# 4) IMPORTANT on C2C platforms: stop the C2C override from re-enabling it
export NCCL_NET_GDR_C2C=0

# 5) Now attempt IB again
export NCCL_NET=IB

# Debug visibility
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=INIT,ENV,NET,REG
```

Why this combo is rational:

* `NCCL_NET_PLUGIN=none` stops NCCL from auto-loading `libnccl-net.so` style plugins. ([NVIDIA Docs][4])
* `NCCL_DMABUF_ENABLE=0` prevents the specific DMABUF path your logs show. ([NVIDIA Docs][4])
* `NCCL_NET_GDR_C2C` **explicitly overrides** `NCCL_NET_GDR_LEVEL` on CPU-attached NIC + C2C systems, and defaults to **1** since 2.27. If you didn’t set this, you may have been “disabling GDR” while NCCL re-enabled it via this override. ([NVIDIA Docs][4])

If this works, you should see logs more like:

* “GPU Direct RDMA Disabled …”
  …and **no crash**, potentially near your **~190 Gb/s** RDMA baseline.

If it still crashes, then yeah: your container’s NCCL stack is fundamentally incompatible with IB on Spark and your `NCCL_NET=Socket` workaround is the pragmatic move.

---

## 6) If you remain on Socket, you might be able to exceed your ~100 Gb/s “cap” with tuning

NCCL socket transport has explicit tuning knobs:

* `NCCL_SOCKET_IFNAME` can include multiple interfaces (comma-separated), exact-match with `=`. ([NVIDIA Docs][4])
* `NCCL_SOCKET_NTHREADS` (1–16) and `NCCL_NSOCKS_PERTHREAD` can increase throughput; NCCL even says for “generic 100G networks” you can try **4** and **4**, with the constraint that the product ≤ **64**. ([NVIDIA Docs][4])

Example:

```bash
export NCCL_NET=Socket
export NCCL_SOCKET_IFNAME="=enp1s0f0np0,enp1s0f1np1"   # adapt to your names
export NCCL_SOCKET_NTHREADS=4
export NCCL_NSOCKS_PERTHREAD=4
```

Will this get you to 200 Gb/s? Maybe, maybe not — sockets become **CPU-bound** quickly — but it’s the only lever set NCCL gives you on that path.

---

## 7) Concrete edits to your report (so it’s maximally accurate)

If you want that writeup to hold up under scrutiny, I’d change three lines:

1. Replace
   “GPUDirect is required for NCCL’s IB mode”
   with
   “In our NGC container stack, the selected NCCL net/IB implementation enables DMABUF and crashes instead of staging.”

2. Add a “things we didn’t try / next knobs” box:

   * `NCCL_NET_PLUGIN=none` ([NVIDIA Docs][4])
   * `NCCL_DMABUF_ENABLE=0` ([NVIDIA Docs][4])
   * `NCCL_NET_GDR_C2C=0` ([NVIDIA Docs][4])

3. Add the authoritative platform statement (NVIDIA forum quote summary):

   * “GPUDirect RDMA not supported; dma-buf / nvidia-peermem don’t work on Spark.” ([NVIDIA Developer Forums][1])

That makes the writeup both **technically correct** and **useful to someone trying to reproduce/fix**.

---

## Popular opinion vs my conclusion

**Popular opinion (generic cluster lore):**
“Use NCCL over IB/RoCE; if GDR isn’t available, disable it and NCCL will stage.”

**My conclusion (what your data + NVIDIA docs/forums actually imply):**

* The “disable GDR and stage” idea is *real* in NCCL, and Spark users have hit **~22 GB/s (~176 Gb/s)** with GDR disabled. ([NVIDIA Developer Forums][2])
* Your crash is almost certainly a **specific plugin/dmabuf selection bug or mis-detection** in the container stack.
* Your `NCCL_NET=Socket` workaround is correct for “make it work now,” but it’s **not the end of the road** if you want to reclaim ~200G inside NCCL.

If you want, paste the exact NCCL version string from your container (`nccl-tests prints it at startup`) and the first ~30 lines of `NCCL_DEBUG=INFO` output — I can tell you whether you’re on internal IB vs IBext/plugin immediately, and which of the above levers is most likely to stick.

[1]: https://forums.developer.nvidia.com/t/enabling-gpu-direct-rdma-for-dgx-spark-clustering/352051 "Enabling GPU Direct RDMA for DGX Spark Clustering - DGX Spark / GB10 - NVIDIA Developer Forums"
[2]: https://forums.developer.nvidia.com/t/dgx-spark-nccl-test-10gb-s-not-200-gbps-25-gb-s/350077 "DGX Spark NCCL Test: 10GB/s not 200 Gbps=25 GB/s - DGX Spark / GB10 - NVIDIA Developer Forums"
[3]: https://github.com/NVIDIA/nccl/issues/676?utm_source=chatgpt.com "Cannot disable IB or force NCCL to use Socket network on ..."
[4]: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html"
[5]: https://build.nvidia.com/spark/nccl/stacked-sparks "NCCL for Two Sparks | DGX Spark"
[6]: https://docs.nvidia.com/deeplearning/nccl/archives/nccl_2243/user-guide/docs/env.html?utm_source=chatgpt.com "Environment Variables — NCCL 2.24.3 documentation"

**TLDR**

* You *can* try to make NCCL use both NICs in **socket mode** by listing both interfaces in `NCCL_SOCKET_IFNAME` (comma-separated, use `=` for exact names). NCCL’s docs explicitly support multi-interface selection. ([NVIDIA Docs][1])
* Whether you’ll actually see traffic on both depends on **IP/routing** (multi-rail subnets make this easy; same-subnet usually needs policy routing or bonding).
* If “IB mode fails because of GPUDirect”, try **IB + no GPUDirect** via `NCCL_NET_GDR_LEVEL=LOC` (forces GDR off). ([NVIDIA Docs][1])

---

## 1) The fastest “just try it” change (socket mode, both IFs)

Right now you pinned to one IF. Instead, whitelist both interfaces explicitly:

```bash
export NCCL_SOCKET_IFNAME="=enp1s0f1np1,=enP2p1s0f1np1"
```

NCCL supports multiple prefixes separated by commas, and `=` forces exact interface-name matching (so you don’t accidentally match other `en*` interfaces). ([NVIDIA Docs][1])

If your two IFs share a common prefix and you *want* prefix-matching, you can also do something like `enp1,enp2`—but exact match is safer.

### Turn on logs so you can see what NCCL actually does

```bash
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=NET,INIT
```

Then run your workload and watch for NCCL printing chosen interfaces / connections.

### Watch per-interface traffic live

```bash
ip -s link show enp1s0f1np1
ip -s link show enP2p1s0f1np1
# or:
sar -n DEV 1
# or:
ifstat -i enp1s0f1np1,enP2p1s0f1np1 1
```

---

## 2) If only one interface still gets traffic: it’s usually routing/topology, not NCCL

Even if NCCL *selects* both interfaces, the kernel can still send everything out one IF if your IP setup is “wrong” for multi-rail.

### The clean setup (recommended): 2 subnets (“multi-rail”)

Example:

* Node A

  * `enp1...` = `192.168.10.11/24`
  * `enP2...` = `192.168.11.11/24`
* Node B

  * `enp1...` = `192.168.10.12/24`
  * `enP2...` = `192.168.11.12/24`

Now Linux has unambiguous routes: `192.168.10.0/24` via IF#1, `192.168.11.0/24` via IF#2.

### The annoying setup (common): both NICs in the same subnet

If both NICs are in the same subnet, Linux routing often picks **one** egress device for that destination prefix. Then your “second rail” never gets used unless you do **policy-based routing** (source routing), or you use **bonding** (next section).

---

## 3) OS-level approach: bond the two ports and point NCCL at the bond

If your switch supports it, an LACP bond is the “make it the network’s problem” solution.

Key reality check:

* A *single* TCP flow usually won’t exceed *one* physical link in LACP (hashing).
* NCCL typically uses multiple connections/sockets, so it *can* spread across links—*especially* if you increase sockets/threads. (See tuning below.)

On Ubuntu with netplan, a sketch:

```yaml
network:
  version: 2
  bonds:
    bond0:
      interfaces: [enp1s0f1np1, enP2p1s0f1np1]
      addresses: [10.10.0.11/24]
      parameters:
        mode: 802.3ad
        lacp-rate: fast
        transmit-hash-policy: layer3+4
        mii-monitor-interval: 100
```

Then:

```bash
export NCCL_SOCKET_IFNAME="=bond0"
```

This can be the most reliable way to “use both” if you don’t want multi-subnet routing complexity.

---

## 4) Socket transport tuning (only if you’re chasing bandwidth)

If you actually want to push towards 100G+ with socket transport, NCCL exposes knobs. NCCL docs explicitly note that for “generic 100G networks” you can set both to 4, with the constraint that the product can’t exceed 64. ([NVIDIA Docs][1])

```bash
export NCCL_SOCKET_NTHREADS=4
export NCCL_NSOCKS_PERTHREAD=4
```

These are per-connection helpers; they can increase CPU usage. ([NVIDIA Docs][1])

Given you measured ~2 Gb/s, this is likely pointless for your real workload—but it’s useful for a bandwidth validation test (e.g., `nccl-tests`).

---

## 5) If you want “both ports” via IB/RDMA but GPUDirect is breaking it

If your IB transport is failing specifically because NCCL is trying to use GPUDirect RDMA, you can force GPUDirect off:

```bash
export NCCL_NET_GDR_LEVEL=LOC
```

`LOC` = “Never use GPU Direct RDMA (always disabled)” per NCCL docs. ([NVIDIA Docs][1])

This can let you still use the IB/RoCE transport path while staging through host memory (performance may still be better than pure TCP, depending on your setup).

---

## 6) One more NCCL knob you should set consciously when using multiple NICs: `NCCL_CROSS_NIC`

This controls whether NCCL allows rings/trees to use different NICs across nodes. ([NVIDIA Docs][1])

* If each NIC is a separate “rail” / separate switch → start with:

  ```bash
  export NCCL_CROSS_NIC=0
  ```
* If both NICs go to the same switch/fabric and rails don’t matter →:

  ```bash
  export NCCL_CROSS_NIC=1
  ```

Default is `2` (try to match rails but allow crossing if beneficial). ([NVIDIA Docs][1])

---

## My conclusion vs the “popular assumption”

* **Conclusion:** You don’t *necessarily* need IB mode or MPTCP to get NCCL to touch multiple NICs. NCCL supports selecting multiple socket interfaces directly (`NCCL_SOCKET_IFNAME` with comma-separated list) and can use all matching interfaces. ([NVIDIA Docs][1])
* **Popular assumption:** “Socket mode = one NIC only; to use both you need bonding/MPTCP.” That’s only true if your IP/routing setup effectively forces one interface, or you’re thinking of *a single TCP flow* needing per-packet multipath.

If you want, I can give you a minimal “2-node verification recipe” (exact `nccl-tests` command + what to look for in NCCL logs) tailored to how many GPUs/ranks you’re running—but the configuration above is the core of “try using both interfaces” without overcomplicating it.

[1]: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html "Environment Variables — NCCL 2.29.1 documentation"




**TLDR**

* Your “IB works in single-node but not in multi-node vLLM” outcome is exactly what I’d expect given Ray’s env model + container device isolation.
* You **don’t need “deep vLLM/Ray integration”** to get multi-node vLLM using **NET/IB with GDR disabled**. You mostly need to:

  1. pass `/dev/infiniband` + memlock into the containers, and
  2. ensure the NCCL env vars exist **when `ray start` runs** on *every node* (not later in a `docker exec` shell / driver). ([NVIDIA Docs][1])
* If you’re only seeing ~2 Gb/s (~0.25 GB/s) in production serving, chasing 22 GB/s RDMA numbers is mostly “feel-good benchmarking,” not a real bottleneck fix.

---

## What’s actually going on (and why your failures make sense)

### 1) `/dev/infiniband/*` missing in containers

That’s normal unless you explicitly pass the devices through (or use `--privileged`). NVIDIA’s own container guidance for RDMA-class networking commonly uses `--device=/dev/infiniband` plus `--ulimit memlock=-1`. ([NVIDIA Docs][1])

### 2) Ray “GPU detection broke” when you wrapped things

Ray GPU detection issues usually come from the container not *really* having GPUs (missing `--gpus all` / wrong runtime) or from clobbering key env (e.g., `CUDA_VISIBLE_DEVICES`, `NVIDIA_VISIBLE_DEVICES`) in a wrapper.

The clean rule: **never “export” inside a shell wrapper to inject cluster-wide env**. Pass env via Docker `-e …` so Ray starts with it, and avoid fragile quoting.

### 3) “Env vars don’t reach Ray workers”

This is not a bug; it’s Ray’s design. Env vars set on the driver / your interactive shell **aren’t automatically propagated** to workers. Ray’s own docs call this out and recommend using **runtime environments** (`runtime_env={"env_vars": {...}}`) when you need propagation. ([Ray][2])

For **vLLM multi-node**, the vLLM docs explicitly recommend setting env vars **during cluster creation** (their `run_cluster.sh`), because that’s what actually propagates to all nodes/processes. ([vLLM][3])

---

## The simplest path to “vLLM multi-node + NCCL IB (GDR off)” without code changes

### Step A — Start Ray containers with RDMA devices and memlock (no `--privileged`)

vLLM’s `examples/online_serving/run_cluster.sh` already forwards arbitrary Docker args you pass (including `-e ...`, `--device ...`, `--ulimit ...`). ([GitHub][4])

On **each node**, use `run_cluster.sh` but add:

* `--device=/dev/infiniband` (or individual uverbs devices)
* `--ulimit memlock=-1`
* (optional but often needed) `--cap-add=IPC_LOCK`
* Your NCCL RDMA env vars as `-e ...`

NVIDIA’s doc example explicitly calls out `--device=/dev/infiniband` and `--ulimit memlock=-1`. ([NVIDIA Docs][1])
vLLM community guidance also mentions adding `IPC_LOCK` and ensuring `/dev/shm` is available for distributed runs. ([vLLM Forums][5])

**Head node:**

```bash
bash run_cluster.sh \
  vllm/vllm-openai \
  192.168.102.11 \
  --head \
  /abs/path/to/hf_cache \
  --device=/dev/infiniband \
  --ulimit memlock=-1 \
  --cap-add=IPC_LOCK \
  -e VLLM_HOST_IP=192.168.102.11 \
  -e NCCL_IB_DISABLE=0 \
  -e NCCL_NET_GDR_LEVEL=LOC \
  -e NCCL_NET_GDR_C2C=0 \
  -e NCCL_DMABUF_ENABLE=0 \
  -e NCCL_NET_PLUGIN=none \
  -e NCCL_DEBUG=INFO \
  -e NCCL_DEBUG_SUBSYS=NET,INIT
```

**Worker node (example worker IP 192.168.102.12):**

```bash
bash run_cluster.sh \
  vllm/vllm-openai \
  192.168.102.11 \
  --worker \
  /abs/path/to/hf_cache \
  --device=/dev/infiniband \
  --ulimit memlock=-1 \
  --cap-add=IPC_LOCK \
  -e VLLM_HOST_IP=192.168.102.12 \
  -e NCCL_IB_DISABLE=0 \
  -e NCCL_NET_GDR_LEVEL=LOC \
  -e NCCL_NET_GDR_C2C=0 \
  -e NCCL_DMABUF_ENABLE=0 \
  -e NCCL_NET_PLUGIN=none \
  -e NCCL_DEBUG=INFO \
  -e NCCL_DEBUG_SUBSYS=NET,INIT
```

Why this works:

* Those env vars exist **when `ray start` runs** in the container, so Ray worker processes inherit them (this is exactly what vLLM warns about). ([vLLM][3])

---

## If you want to use both “halves” in IB mode: set `NCCL_IB_HCA` correctly

For multi-rail / multi-port IB (or RoCE), you usually need to include both HCAs/ports in `NCCL_IB_HCA`. NCCL supports comma-separated selection, port selection with `:`, and `=` for exact names. ([NVIDIA Docs][6])

1. Map devices:

```bash
ibdev2netdev
# or: rdma link show
```

2. Then set something like:

```bash
-e NCCL_IB_HCA="=mlx5_0:1,mlx5_1:1"
```

(Use your actual device names/ports from `ibdev2netdev`.)

---

## How to verify you’re actually on NET/IB (with GDR disabled)

Inside the head container (or wherever you run `vllm serve`), set:

```bash
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=NET,INIT
```

Then look for logs indicating `NET/IB` and that GDR is disabled (you already saw the pattern in your single-node test). If you want “belt and suspenders,” temporarily crank to `TRACE` for one run.

---

## One correction to your conclusion

You wrote: “vLLM multi-node requires Socket due to Ray env propagation complexity.”

It’s only “complex” if you try to set NCCL env vars *after* the Ray cluster is already started (e.g., in a `docker exec` shell or only in the driver). Ray explicitly doesn’t propagate driver env vars to workers unless you use runtime_env. ([Ray][2])
vLLM explicitly tells you to set env vars at cluster creation (run_cluster.sh) for this reason. ([vLLM][3])

So: **IB is achievable without touching vLLM source**. It’s mostly “start the cluster correctly.”

---

## Should you bother, given ~2 Gb/s real traffic?

Tell-it-like-it-is: probably not right now.

* Your serving run is using **~2 Gb/s ≈ 0.25 GB/s**.
* Even a single 100G NIC gives **~12.5 GB/s theoretical**, i.e. **~50× headroom**.
* Yes, your benchmark table says **RDMA (GDR off) ~22 GB/s** vs socket **4–6 GB/s**; that’s a *real* transport difference. But it’s irrelevant unless your workload’s communication grows by ~20–80×.

Forward-looking rule: only pay the RDMA complexity tax if you expect one of these soon:

* more nodes (2 → 4 → 8+) with heavy TP/PP,
* smaller/faster kernels where comm dominates,
* you’re chasing latency jitter / tail latency at high QPS,
* you want to run other workloads where comm is the bottleneck.

If not, keep socket mode, document the “known-good RDMA recipe” above, and move on.

If you want, I can also rewrite your CLAUDE.md snippet into a tighter “decision log” that makes the Ray/env nuance explicit and avoids the slightly-too-pessimistic “needs deep integration” framing.

[1]: https://docs.nvidia.com/networking/display/xliov360/setting-up-xlio-within-a-docker-container?utm_source=chatgpt.com "Setting Up XLIO Within a Docker Container"
[2]: https://docs.ray.io/en/latest/ray-observability/user-guides/debug-apps/general-debugging.html "Common Issues — Ray 2.53.0"
[3]: https://docs.vllm.ai/en/stable/serving/distributed_troubleshooting/ "Troubleshooting distributed deployments - vLLM"
[4]: https://raw.githubusercontent.com/vllm-project/vllm/main/examples/online_serving/run_cluster.sh "raw.githubusercontent.com"
[5]: https://discuss.vllm.ai/t/deploying-multi-node-llm-with-infiband-roce/1344 "Deploying multi-node LLM with infiband/ROCE - General - vLLM Forums"
[6]: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html?utm_source=chatgpt.com "Environment Variables — NCCL 2.29.1 documentation"

### **TLDR**

* Yes—**for interactive chat, the network mostly shows up as *token-to-token latency*** (TPOT), not peak bandwidth.
* But with your observed **~2 Gb/s** real traffic, you’re **nowhere near** even a single 100G port’s bandwidth ceiling, so **RDMA + 200G won’t magically double throughput**. Expect **single‑digit to low‑teens % improvement** in decode latency/throughput *at best* (bigger gains only if you’re actually comm‑bound).
* A real-world anchor: someone running **Qwen3‑VL‑235B‑A22B AWQ on a 2‑node DGX Spark cluster reports ~25 tokens/s** generation throughput. ([NVIDIA Developer Forums][1])
* “Proper” dual‑port RoCE/NCCL on Spark can hit **~26.9 GB/s NCCL bus bandwidth (~215 Gb/s)** in benchmarks, i.e. the fabric can be *very* fast when you’re driving it. ([LinkedIn][2])
* Ballpark (educated) expectation for **your current sockets/1×100G vs RDMA/200G**:

  * **Batch 1 decode:** ~**25 tok/s → ~28–30 tok/s**
  * **Prefill/encode:** changes are usually smaller (often **~0–10%**), unless you’re doing very heavy multi-node prefill.

---

## What “latency vs bandwidth” means for Qwen‑235B inference

Think in two phases:

### 1) Encode / prefill (processing the prompt)

* Big matmuls, often **compute‑heavy**.
* Network can matter if you’re doing **multi-node tensor-parallel** and the implementation forces big cross-node collectives during prefill.
* But if your measured traffic is ~2 Gb/s, **you’re not bandwidth-limited**; RDMA might shave a bit of overhead, but it’s not a night-and-day change.

### 2) Decode (generating tokens)

* **Every token step** tends to require **synchronization/collectives** across tensor-parallel ranks.
* That makes **latency** more relevant: even tiny per-collective overhead gets multiplied by “number of collectives per token”.
* Evidence that latency matters: in dual-Spark llama.cpp RPC testing, the author explicitly notes TCP/IP stack latency as a limiter and references **~1–2 µs RDMA latency** (`ib_send_lat`). ([GitHub][3])

So yes: **if you care about interactive feel (TPOT), RDMA helps more than extra raw bandwidth**.

---

## Baseline reality check: what people actually see on similar setups

### “Current-ish” throughput on dual Spark / multi-node setups

* **~25 tokens/sec** reported for **Qwen3‑VL‑235B‑A22B Instruct AWQ on a 2‑node cluster in vLLM**. ([NVIDIA Developer Forums][1])
* For a different stack (llama.cpp RPC, Q4_K), measured **token generation ~12.98 tok/s**, and it **drops with longer context** (down to ~8 tok/s at 32K). ([GitHub][3])

This wide spread is normal: engine + quantization + scheduling dominate.

### “Proper RDMA 200G” capability on Spark

* Dual‑port RoCE/NCCL tests show traffic can be balanced and NCCL can reach **~26.9 GB/s bus bandwidth (~215 Gb/s)** under the right settings. ([LinkedIn][2])
  So the network *can* be fast—question is whether **your inference workload actually hits that wall**. Your ~2 Gb/s observation says “no”.

---

## Educated guesses: encode + decode vs batch size (Socket/100G vs RDMA/200G)

I’m going to assume the scenario you’ve been describing:

* **2 nodes, tensor-parallel across nodes**, Qwen3‑VL‑235B‑A22B (AWQ-ish), vLLM-style serving
* “Batch size” = concurrent sequences being decoded together (what vLLM effectively batches)
* Context ~4K-ish (not 32K); at 32K, decode will be noticeably slower regardless (attention/KV cost) ([GitHub][3])

These numbers are *not* gospel—they’re meant to be **plausible** and **directionally correct** given (a) your traffic is tiny vs link capacity and (b) decode is latency sensitive.

### Table: estimated throughput + per-seq token latency

**Decode throughput = output tokens/sec.**
**Prefill throughput = input tokens/sec.**
Per-seq token latency ≈ `1000 * batch / total_decode_tps` ms.

| Batch (seqs) | Socket 1×100G Prefill t/s | Socket Decode t/s (total) | Socket ms/token per seq | RDMA 200G Prefill t/s | RDMA Decode t/s (total) | RDMA ms/token per seq |
| -----------: | ------------------------: | ------------------------: | ----------------------: | --------------------: | ----------------------: | --------------------: |
|            1 |                      ~400 |                   **~25** |                  ~40 ms |                  ~420 |                 **~29** |                ~34 ms |
|            2 |                      ~700 |                       ~45 |                  ~44 ms |                  ~735 |                     ~50 |                ~40 ms |
|            4 |                     ~1200 |                       ~80 |                  ~50 ms |                 ~1300 |                     ~86 |                ~47 ms |
|            8 |                     ~1800 |                      ~130 |                  ~62 ms |                 ~1950 |                    ~137 |                ~58 ms |
|           16 |                     ~2500 |                      ~200 |                  ~80 ms |                 ~2700 |                    ~206 |                ~78 ms |

**How to read this:**

* **Throughput goes up with batch**, but **interactive latency per user usually gets worse** (ms/token rises).
* **RDMA helps most at small batch** (latency-dominated), and becomes marginal at larger batch (compute-dominated).
* The anchor point (**~25 tok/s at low concurrency**) is consistent with a reported dual-node vLLM AWQ result. ([NVIDIA Developer Forums][1])
* The “RDMA can help but not 10×” stance is consistent with the fact that Spark RoCE can be extremely fast in NCCL microbenchmarks, but your workload isn’t pushing bandwidth. ([LinkedIn][2])

**Expected deltas (rule of thumb):**

* **Batch 1 decode:** +10–20% is plausible
* **Batch 8+ decode:** +0–5% is plausible
* **Prefill:** often +0–10% unless your prefill is unusually communication-heavy

---

## Popular opinion vs my conclusion

* **Popular opinion:** “Multi-node tensor-parallel LLM inference *needs* RDMA; TCP sockets are too slow.”
* **My conclusion:** RDMA is *nice*, but in your observed regime (**~2 Gb/s traffic**), **you are not bandwidth-limited**, so the *only* meaningful gain is **lower collective latency/jitter**, which typically yields **modest** improvements (single digits to low teens) in **decode TPOT**, not a dramatic throughput jump.

---

## If you want real numbers in 10 minutes (and stop guessing)

Run two controlled microbenchmarks and record **prefill** + **decode** separately:

1. **Fix prompt+output lengths** (example: input 1024, output 256)
2. Sweep **batch size** (1,2,4,8,16)
3. Compare:

* **Socket path**: `NCCL_IB_DISABLE=1`
* **RDMA path**: `NCCL_IB_DISABLE=0` plus your IB/GDR-off settings

Also: NCCL *can* be configured to consider multiple NICs (comma-separated selection and multi-NIC behavior controls exist). ([NVIDIA Docs][4]) But whether you’ll see both links used for *your* pattern (especially with 1 GPU/node) is a separate practical question.

If you paste your vLLM `Avg prompt throughput` / `Avg generation throughput` logs for a single controlled run (with known batch and fixed lengths), I can replace the guessed table with a tighter estimate that matches your exact setup.

[1]: https://forums.developer.nvidia.com/t/question-on-inference-performance-results-of-qwen3-235b-a22b-on-2x-dgx-spark/355053?utm_source=chatgpt.com "Question on Inference Performance Results of Qwen3 ..."
[2]: https://www.linkedin.com/pulse/connecting-two-sparks-andrew-myers-p9gme "https://www.linkedin.com/pulse/connecting-two-sparks-andrew-myers-p9gme"
[3]: https://github.com/ggml-org/llama.cpp/discussions/16578 "Performance of llama.cpp on NVIDIA DGX Spark · ggml-org llama.cpp · Discussion #16578 · GitHub"
[4]: https://docs.nvidia.com/deeplearning/nccl/archives/nccl_2234/user-guide/docs/env.html "https://docs.nvidia.com/deeplearning/nccl/archives/nccl_2234/user-guide/docs/env.html"

**TLDR**

* Yes: for TP=2 inference, this is mostly a **latency** problem (many small-ish collectives), not a bandwidth problem. **RDMA helps a lot** because it cuts software/CPU overhead and tail latency; **100G → 200G dual‑rail usually helps very little** unless you push big decode batches or very long prompt-prefill.
* Real baseline numbers people see on **2× DGX Spark** for **Qwen3‑VL‑235B AWQ in vLLM** are ~**25–26 tok/s** (low concurrency) over **NCCL/RDMA**. ([NVIDIA Developer Forums][1])
* Your measured ratio (**242 vs 167 tok/s**, +45%) is exactly the kind of win you get from **Socket → RDMA** when you’re latency-bound. Expect **single‑rail 100G RDMA vs “true” 200G RDMA** to be more like **0–10%** for decode latency, not 2×.
* If you still want NCCL to use both RoCE “halves”: **stop inferring it from `ip -s link` deltas** and force/prove it with NCCL logs: `NCCL_DEBUG_FILE=/tmp/nccl.%h.%p.log` + `NCCL_NETDEVS_POLICY=ALL` (or `MAX:2`) + exact `NCCL_IB_HCA`. ([NVIDIA Docs][2])

---

## Latency vs bandwidth: what changes between Socket, 100G RDMA, and 200G RDMA

For **TP=2**, every generated token triggers a bunch of collectives. The per‑collective message sizes are often *not huge*, so:

* **Socket transport**: higher per‑collective overhead → worse token latency.
* **RDMA (even with GDR off)**: lower overhead → *much* better token latency (your +45% matches this pattern).
* **Dual‑rail 200G**: mostly increases peak bandwidth; it doesn’t magically cut per‑collective fixed costs. So it tends to help **throughput-at-large-batch** more than **latency-at-batch=1**.

Popular “folk wisdom” here is basically right: **people overestimate bandwidth and underestimate latency** for TP inference.

---

## Throughput numbers you can actually anchor on (public + yours)

### Public datapoints (DGX Spark / 2 nodes)

* **vLLM + NCCL/RDMA, TP=2**: community-reported decode throughput for **Qwen3‑VL‑235B** is about **~25–26 tok/s** (cluster, low concurrency). ([NVIDIA Developer Forums][1])
* **TensorRT‑LLM NVFP4 (Qwen3‑235B, not VL)**: NVIDIA’s published bench shows **prompt/prefill ~23,477 tok/s** and **decode ~11.73 tok/s** at **ISL=2048, OSL=128, BS=1** on **dual DGX Spark**. ([NVIDIA Developer][3])
  (Different engine + different model variant, but it’s a useful “shape of reality”: prefill can be *orders* faster than decode.)

### Your datapoint (most useful for *your* stack)

* **Socket vs RDMA**: **167 → 242 tok/s** (≈**1.45×**).
  That ratio is exactly what you’d expect when comm is latency‑dominated and RDMA removes a big chunk of overhead.

---

## Educated decode estimates by batch size (your current vs “proper” 200G)

I’m going to interpret “batch size” the way vLLM people usually mean it operationally: **effective decode batch / number of sequences decoded per step** (often driven by concurrency / scheduler). Total tok/s goes up, but **per‑request latency can get worse**.

Below is a *calibrated guess* that matches:

* public “batch≈1” reality (~26 tok/s RDMA) ([NVIDIA Developer Forums][1])
* your “bigger batch” observation (~242 tok/s RDMA)

### Decode throughput (total tok/s across all sequences)

| Batch (decode) | Socket ~1×100G total tok/s | RDMA ~1×100G total tok/s | RDMA dual‑rail ~2×100G total tok/s (range) |
| -------------: | -------------------------: | -----------------------: | -----------------------------------------: |
|              1 |                      ~17.9 |                    ~26.0 |                                 ~26.0–26.8 |
|              2 |                      ~31.3 |                    ~45.4 |                                 ~45.9–47.2 |
|              4 |                      ~54.7 |                    ~79.3 |                                 ~80.9–84.1 |
|              8 |                      ~95.6 |                   ~138.5 |                               ~142.7–149.6 |
|             16 |    **167** *(your Socket)* |    **242** *(your RDMA)* |                                   ~254–266 |

**What this means for latency (per sequence):**
Total tok/s is *not* the latency metric. Per‑sequence decode rate ≈ (total tok/s) / (batch).

Examples (approx):

* **Batch=1**

  * Socket: 17.9 tok/s ⇒ **~56 ms/token**
  * RDMA: 26 tok/s ⇒ **~38 ms/token**
* **Batch=16**

  * Socket: 167/16=10.4 tok/s ⇒ **~96 ms/token**
  * RDMA: 242/16=15.1 tok/s ⇒ **~66 ms/token**

So if you crank batch/concurrency for throughput, don’t be surprised if interactive latency gets worse even though “tok/s” looks better.

### Why 200G dual‑rail barely moves decode at batch=1

Even if dual‑rail doubles bandwidth, you’re not bandwidth-limited in decode at low batch; you’re paying a lot of fixed per-collective cost. So you typically see **single-digit %** changes, unless you deliberately push large decode batches.

---

## Prefill (encode) speed: what to expect

Prefill depends brutally on:

* prompt length (ISL),
* how many sequences you batch,
* attention kernel quality,
* whether you’re doing vision encoder work,
* quant type (AWQ vs NVFP4).

Hard datapoint you can cite:

* TRT‑LLM NVFP4 on dual Sparks (Qwen3‑235B): **~23,477 prompt tok/s** at **ISL=2048, BS=1**. ([NVIDIA Developer][3])
  That implies ~**87 ms** to encode a 2048‑token prompt (2048 / 23477).

For **vLLM + AWQ**, I would expect **lower** prefill tok/s than TRT‑LLM for the same setup (engine matters), but still “high enough” that **decode dominates end‑to‑end latency** for typical chat prompts. The network mode (100G vs 200G) tends to matter **more for prefill at high batch / long prompts** than for decode-at-batch=1.

---

## Your NCCL dual‑NIC questions (1–10)

### 1) “TP=2 = 1 connection = 1 NIC?”

Not strictly. NCCL can use **multiple HCAs even for 2 ranks** by mapping different **channels** to different rails. There’s a DGX Spark example showing channels going via **NET/IB/0** and **NET/IB/1** in the NCCL logs (same 2‑node pattern you have). ([NVIDIA Developer Forums][4])
So your hypothesis is plausible as a *default heuristic*, but it’s not a hard rule.

### 2) `NCCL_IB_HCA` format + “omit :1?”

Your syntax is basically fine:

* `NCCL_IB_HCA` accepts a **comma-separated list** of HCAs, and you can specify a port with `:port`. ([NVIDIA Docs][2])
* The doc also recommends using a leading `=` **in the value** to force *exact* matching (avoid prefix accidents): e.g. `=mlx5_0:1,mlx5_1:1`. ([NVIDIA Docs][2])

So for you, I’d do:

```bash
export NCCL_IB_HCA="=rocep1s0f1:1,roceP2p1s0f1:1"
```

Omitting `:1` usually just means “all ports”; if each device only has port 1, it won’t change much.

### 3) `NCCL_IB_SPLIT_DATA_ON_QPS` to distribute across both NICs?

This knob is about **how data is split across multiple QPs** *once you already have multiple QPs*, controlled by `NCCL_IB_QPS_PER_CONNECTION`. ([NVIDIA Docs][2])
It’s **not** the primary “use both NICs” switch.

Also: NCCL explicitly warns that setting `NCCL_IB_SPLIT_DATA_ON_QPS=1` (splitting each message across QPs) can cause **visible latency degradation** when using many QPs. ([NVIDIA Docs][2])
Given you care about latency, I would *not* reach for this first.

### 4) Different subnets (192.168.100.x vs 192.168.101.x): problem?

For RoCE multi‑rail this is generally fine *as long as both nodes have connectivity on both subnets*. NCCL is selecting IB devices/ports; the IP subnet split is not inherently disqualifying.

### 5) `NCCL_ALGO` (Ring/Tree) affecting NIC selection?

For **2 ranks**, Ring vs Tree is basically a degenerate case; it’s not going to be the lever that suddenly makes 2 rails appear. It can change micro-behavior, but if you’re seeing “only one rail”, fix device selection first.

### 6) Where do NCCL logs go in vLLM/Ray?

Two practical points:

1. NCCL logs go to **stdout/stderr by default**, and Ray often captures/redirects worker output.
2. Use `NCCL_DEBUG_FILE` and write per‑process logs to disk. NCCL supports `%h` (hostname) and `%p` (PID) in the filename. ([NVIDIA Docs][2])

Example:

```bash
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=INIT,NET
export NCCL_DEBUG_FILE=/tmp/nccl.%h.%p.log
```

Then look in Ray’s log tree (commonly `/tmp/ray/session_latest/...`). ([Ray][5])

### 7) “Latency benefit” from dual‑NIC even at 2–5 Gb/s?

Usually **no**, or it’s lost in noise.

* Dual‑rail mainly improves bandwidth and reduces queueing under load.
* When you’re far from saturating 100G, you’re dominated by per‑collective overhead.
* Aggressive QP splitting can even **hurt latency** (NCCL warns about this). ([NVIDIA Docs][2])

### 8) ConnectX‑7 multi‑host mode: known NCCL multi‑rail issues?

On DGX Spark specifically, there’s evidence NCCL can use both `rocep1s0f1` and `roceP2p1s0f1` rails (channels mapped across both in logs). ([NVIDIA Developer Forums][4])
So I wouldn’t blame “multi‑host mode” as the first culprit.

### 9) Does vLLM/Ray prevent NCCL multi‑NIC?

Not conceptually. The real foot-gun is **env propagation** into Ray worker processes (which you already hit earlier). If the workers don’t see the same NCCL env, they’ll silently do something else.

### 10) Is single‑NIC usage “expected” for 2‑node TP?

It’s *common* with defaults/heuristics, but not inevitable. NCCL can use both rails; you just need to **force/verify** selection and make sure the env reaches the right processes.

---

## The “do this, not vibes” recipe to force dual‑rail and prove it

### 1) Force NCCL to *consider* both rails and log to files

```bash
# pick rails
export NCCL_IB_HCA="=rocep1s0f1:1,roceP2p1s0f1:1"
export NCCL_CROSS_NIC=0              # avoid cross-rail weirdness :contentReference[oaicite:15]{index=15}

# force NCCL to use multiple net devices per GPU
export NCCL_NETDEVS_POLICY=ALL       # or MAX:2 :contentReference[oaicite:16]{index=16}

# logging you can actually find under Ray
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=INIT,NET
export NCCL_DEBUG_FILE=/tmp/nccl.%h.%p.log   # %h host, %p pid :contentReference[oaicite:17]{index=17}
```

### 2) Don’t accidentally pin Socket selection to one interface (if you ever fall back)

`NCCL_SOCKET_IFNAME` supports **multiple interfaces** (comma-separated) and exact matching with `=`. ([NVIDIA Docs][2])
So if you keep it at all, prefer:

```bash
export NCCL_SOCKET_IFNAME="=enp1s0f1np1,enP2p1s0f1np1"
```

(Or unset it entirely while testing IB behavior, to avoid self-inflicted constraints.)

### 3) Verify via NCCL logs, not `ip -s link`

In the NCCL logs you want to see lines like:

* `NET/IB : Using [0]rocep1s0f1:1/... [1]roceP2p1s0f1:1/...`
* channel mappings across `NET/IB/0` and `NET/IB/1`
  …like the DGX Spark example does. ([NVIDIA Developer Forums][4])

---

If you want one blunt “next step”: **try `NCCL_NETDEVS_POLICY=ALL` + `NCCL_DEBUG_FILE` and see if NCCL even *tries* to open both rails**. If it does, dual‑NIC is basically solved; if it doesn’t, the problem is still env‑propagation or device filtering (not “TP=2 can’t do it”).

[1]: https://forums.developer.nvidia.com/t/how-to-run-glm-4-7-on-dual-dgx-sparks-with-vllm-mods-support-in-spark-vllm-docker/355603/16 "https://forums.developer.nvidia.com/t/how-to-run-glm-4-7-on-dual-dgx-sparks-with-vllm-mods-support-in-spark-vllm-docker/355603/16"
[2]: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html "Environment Variables — NCCL 2.29.1 documentation"
[3]: https://developer.nvidia.com/blog/how-nvidia-dgx-sparks-performance-enables-intensive-ai-tasks/ "https://developer.nvidia.com/blog/how-nvidia-dgx-sparks-performance-enables-intensive-ai-tasks/"
[4]: https://forums.developer.nvidia.com/t/help-running-nvfp4-model-on-2x-dgx-spark-with-vllm-ray-multi-node/353723 "Help: Running NVFP4 model on 2x DGX Spark with vLLM + Ray (multi-node) - DGX Spark / GB10 - NVIDIA Developer Forums"
[5]: https://discuss.ray.io/t/ray-log-location/10394 "https://discuss.ray.io/t/ray-log-location/10394"

**TLDR**
Yes — on DGX Spark specifically, a “dual‑rail” setup can be *worse* than a single ~100 Gb rail if it’s the wrong kind of dual‑rail (bonding/load‑balancing that doesn’t actually spread your traffic), or if you accidentally pin both “rails” onto the same underlying PCIe path. With Spark’s ConnectX‑7 wiring/port mapping, it’s easy to end up at ~92–98 Gbps when you thought you built 200 Gbps, and you can also add latency/instability. If you just want a fast, simple 2‑node direct link for NCCL: one QSFP cable + NVIDIA’s playbook is usually the least painful way to hit “full” bandwidth; dual‑cable/dual‑rail only helps if you can validate all interfaces end‑to‑end.

---

## Why dual‑rail can be worse on DGX Spark (not just “not better”)

### 1) Spark’s “200 Gb” is not a normal “one link = 200 Gb” story

DGX Spark lists a ConnectX‑7 NIC at **200 Gbps**. ([NVIDIA][1])
But in practice, the NIC shows up as **4 network interfaces** in Linux even though the chassis has **2 QSFP ports**. NVIDIA engineers explain this is because **GB10 can’t provide more than PCIe x4 per device**, so they use **ConnectX‑7 multi‑host mode** to aggregate **two separate PCIe Gen5 x4 links** to reach 200 Gbps. ([NVIDIA Developer Forums][2])

Net effect: you can easily build a “dual‑rail” that *looks* right but doesn’t actually give you two independent 100 Gb paths.

### 2) Mis-pairing the “rails” can cap you at ~100 Gb (or a bit below)

ServeTheHome’s deep dive shows the underlying wiring is finicky: if you “mess this up” you end up driving **two links through one PCIe Gen5 x4** and you land around **~92–95 Gbps**; with careful alignment + RDMA tooling they measured about **~185–190 Gbps** combined. ([ServeTheHome][3])

So yes: **dual‑rail can be worse than single‑rail** if your dual‑rail accidentally collapses onto the same bottleneck (one x4 path) *and* adds overhead.

### 3) Bonding/LACP often doesn’t do what people assume (especially for “one big flow”)

If your “direct dual connection” is mostly:

* one or a few big TCP flows (scp/rsync/NFS/iperf with default settings), or
* any workload that doesn’t create lots of parallel flows/channels

…then typical link aggregation (LACP, bonding mode 802.3ad, XOR hashing, etc.) may place most traffic on **one** member link. That can be **equal to** a single 100 Gb rail, or **worse** if you add:

* packet reordering (certain bonding modes),
* extra CPU/interrupt overhead,
* suboptimal hash distribution.

This is exactly why people report “link is 200G, iperf is ~98G” on Spark. ([NVIDIA Developer Forums][2])

### 4) RDMA/NCCL can fall back or underutilize rails if you “help” it the wrong way

For NCCL/UCX:

* If you inadvertently force NCCL onto sockets/TCP, you can lose a lot versus RDMA.
* If you bind UCX/NCCL to only one interface, you may cap at ~100.
  NVIDIA’s “NCCL for Two Sparks” walkthrough explicitly binds `UCX_NET_DEVICES` / `NCCL_SOCKET_IFNAME` to a single active interface in the example. ([build.nvidia.com][4])
  That may be fine for their intended setup, but it also means **your dual‑rail won’t automatically help** unless the rest of the stack is actually using both rails.

---

## What NVIDIA’s docs imply for your exact scenario (direct 2‑node link)

NVIDIA’s own “Connect Two Sparks” playbook says:

* **One QSFP cable** between the two Sparks is enough for “200GbE direct connection”. ([build.nvidia.com][5])
* **Full bandwidth can be achieved with just one QSFP cable.**
* If you use **two QSFP cables**, then **all four interfaces must be assigned IP addresses to obtain full bandwidth**. ([build.nvidia.com][5])

So from a “don’t overcomplicate it” standpoint, the docs are basically saying:
**single‑cable is the default best path; dual‑cable/dual‑rail is advanced and easy to botch.**

---

## My conclusion vs popular opinion

**Popular opinion (HPC / cluster world):**
Dual‑rail is generally better (more bandwidth, resilience, more paths).

**My conclusion (for DGX Spark direct 2‑node):**
Dual‑rail is *conditionally* better, but **it’s unusually easy for “dual rail” on Spark to become worse than a clean single 100 Gb rail** because:

* the 200 Gb story is implemented via multi‑host / split interfaces (not a simple single netdev),
* misconfiguration can hard-cap you at ~92–98 Gbps ([ServeTheHome][3]),
* bonding/load balancing can underutilize or destabilize things,
* many workloads won’t naturally spread traffic.

If you want **reliable performance quickly**, start with **one QSFP cable** and validate NCCL performance; only then add complexity.

---

## Practical guidance (no fluff)

### If your goal is NCCL/distributed training performance

1. Do **single QSFP cable**, follow NVIDIA’s “Connect Two Sparks” steps. ([build.nvidia.com][5])
2. Validate with `nccl-tests` and look at **bus bandwidth** (rule of thumb):

* 100 Gbps ≈ **12.5 GB/s** theoretical (≈11.6 GiB/s)
* 200 Gbps ≈ **25 GB/s** theoretical (≈23.3 GiB/s)
  If you’re seeing ~12 GB/s class numbers, you’re effectively on one “100G half”.

### If you insist on dual‑rail / dual‑cable

* Treat it as an **advanced tuning exercise**, not “plug 2 cables = 2× faster”.
* Per NVIDIA: you must assign IPs to **all four interfaces** to get full bandwidth with two cables. ([build.nvidia.com][5])
* Expect **~200 Gbps total**, not 400, because the underlying limitation is the PCIe delivery into the NIC. ([NVIDIA Developer Forums][2])

### Quick sanity checks that catch 90% of “dual rail is worse” cases

* `ibdev2netdev` to see what’s actually **Up** (NVIDIA uses this in their playbooks). ([build.nvidia.com][5])
* Don’t trust a single `iperf3` stream. Use lots of parallelism (e.g., `-P 32` or more) if you’re testing aggregate throughput; otherwise you can “prove” the link is 100 even when the platform can do ~200. ([ServeTheHome][3])

---

If you tell me what traffic you actually care about (NCCL collectives? NFS? model shard transfer? RDMA perftest?), I can be more specific about whether dual‑rail helps or just adds failure modes. But the direct answer is: **yes, dual‑rail can absolutely be worse than a single 100 Gb rail on Spark — and it’s not rare; it’s a common misconfiguration outcome on this platform.**

[1]: https://www.nvidia.com/en-us/products/workstations/dgx-spark/ "A Grace Blackwell AI supercomputer on your desk | NVIDIA DGX Spark"
[2]: https://forums.developer.nvidia.com/t/connectx-7-nic-in-dgx-spark/350417 "ConnectX-7 NIC in DGX Spark - DGX Spark / GB10 - NVIDIA Developer Forums"
[3]: https://www.servethehome.com/the-nvidia-gb10-connectx-7-200gbe-networking-is-really-different/ "The NVIDIA GB10 ConnectX-7 200GbE Networking is Really Different - ServeTheHome"
[4]: https://build.nvidia.com/spark/nccl/stacked-sparks "NCCL for Two Sparks | DGX Spark"
[5]: https://build.nvidia.com/spark/connect-two-sparks/stacked-sparks "Connect Two Sparks | DGX Spark"

**TLDR**
Your script is *not* “dual‑rail RDMA” right now: you explicitly force **single‑rail** with `NCCL_IB_HCA='=rocep1s0f1:1'`, and you also have a **mismatched control‑plane interface vs Ray IP** (`OOB_IF=enp1s0f1np1` but `HEAD_IP/WORKER_IP=192.168.100.x`). Fix both, then dual‑rail is basically: **pick one QSFP port (f0 or f1), keep the IP + IFNAME consistent, and set `NCCL_IB_HCA` to include both “twin” RoCE devices for that port** (e.g., `=rocep1s0f1:1,roceP2p1s0f1:1`). Don’t bond unless you enjoy pain; the most reliable path on Spark is multi‑rail without bonding. ([NVIDIA NIM APIs][1])

---

## The big problems in your script (straight)

### 1) You’ve got a **routing/identity mismatch** (Ray vs IFNAME)

You set:

* `OOB_IF="enp1s0f1np1"`
* `HEAD_IP="192.168.100.10"` / `WORKER_IP="192.168.100.11"`

…but NVIDIA’s own manual netplan example maps **192.168.100.10/11 to `enp1s0f0np0`**, while `enp1s0f1np1` is on **192.168.200.12/13**. ([NVIDIA NIM APIs][1])

So you’re effectively telling:

* Ray: “my node IP is 192.168.100.x (f0)”
* NCCL/Gloo bootstrap: “use enp1s0f1np1 (f1)”

That can work accidentally, but it’s an easy way to get “it starts but comms are weird” failure modes.

### 2) You explicitly force **single rail**

This line:

```bash
ENV="$ENV -e NCCL_IB_HCA='=rocep1s0f1:1'"
```

means: use **exactly one RDMA interface/port**. The leading `=` is valid NCCL syntax (“exact match”) and `:1` picks port 1. ([NVIDIA Docs][2])
But it also guarantees you’ll never get dual‑rail bandwidth.

### 3) Your “dual‑rail intent” conflicts with the official Spark guidance (and the reality)

NVIDIA’s Spark docs repeatedly emphasize there are “twin” interfaces per physical QSFP port (the `enp1…` and the `enP2…` side). ([NVIDIA NIM APIs][1])
If you want real aggregate bandwidth, you need to treat those as separate rails at the RDMA level (or rely on NCCL fusion). People trying Linux bonding often report it either breaks RoCE reachability or only uses one link per flow; multi‑rail is what’s stable. ([NVIDIA Developer Forums][3])

---

## What “dual rail while keeping RDMA” should look like in your script

### Step 1) Choose **one** QSFP port and make everything consistent

You have two sane configurations:

#### Option A: Use **f0** (192.168.100.x)

Matches NVIDIA’s manual example for `enp1s0f0np0`. ([NVIDIA NIM APIs][1])
Use:

* `OOB_IF=enp1s0f0np0`
* `HEAD_IP=192.168.100.10`
* `WORKER_IP=192.168.100.11`
* Dual‑rail RoCE devices: `rocep1s0f0` and `roceP2p1s0f0`

#### Option B: Use **f1** (192.168.200.x)

Matches NVIDIA’s manual example for `enp1s0f1np1`. ([NVIDIA NIM APIs][1])
Use:

* `OOB_IF=enp1s0f1np1`
* `HEAD_IP=192.168.200.12`
* `WORKER_IP=192.168.200.13`
* Dual‑rail RoCE devices: `rocep1s0f1` and `roceP2p1s0f1`

### Step 2) Set `NCCL_IB_HCA` to **both rails**

NCCL supports listing multiple HCAs/ports; with `=` it treats tokens as exact interface names. ([NVIDIA Docs][2])

For f0:

```bash
ENV="$ENV -e NCCL_IB_HCA='=rocep1s0f0:1,roceP2p1s0f0:1'"
```

For f1:

```bash
ENV="$ENV -e NCCL_IB_HCA='=rocep1s0f1:1,roceP2p1s0f1:1'"
```

### Step 3) Add 1–2 knobs that actually help multi‑rail saturate

On Spark specifically, people have seen traffic “look pinned” to one port unless you increase QP parallelism. One practical suggestion from the Spark forum is:

* `NCCL_IB_QPS_PER_CONNECTION=4` (or 8 to test) ([NVIDIA Developer Forums][3])

So add:

```bash
ENV="$ENV -e NCCL_IB_QPS_PER_CONNECTION=4"
```

If you want NCCL to *force* fusing devices (so it stripes more predictably), NCCL 2.24+ added `NCCL_NET_FORCE_MERGE`, where you describe fused NIC groups as a semicolon-delimited list of comma-delimited NIC names. ([NVIDIA Developer][4])
Example (for f1 twins):

```bash
ENV="$ENV -e NCCL_NET_FORCE_MERGE=rocep1s0f1,roceP2p1s0f1"
```

This is optional; try without first.

---

## Concrete patch for your script (drop-in)

Here’s the minimum change set that makes it coherent + dual‑rail.

### Replace your current network block with this

```bash
# Pick which QSFP port you’re using: f0 (192.168.100.x) or f1 (192.168.200.x)
PORT="${PORT:-f0}"

if [[ "$PORT" == "f0" ]]; then
  OOB_IF="enp1s0f0np0"
  HEAD_IP="192.168.100.10"
  WORKER_IP="192.168.100.11"
  NCCL_HCA="=rocep1s0f0:1,roceP2p1s0f0:1"
elif [[ "$PORT" == "f1" ]]; then
  OOB_IF="enp1s0f1np1"
  HEAD_IP="192.168.200.12"
  WORKER_IP="192.168.200.13"
  NCCL_HCA="=rocep1s0f1:1,roceP2p1s0f1:1"
else
  echo "PORT must be f0 or f1"
  exit 1
fi
```

(Those IP↔IFNAME pairings come straight from NVIDIA’s “Connect Two Sparks” manual netplan example.) ([NVIDIA NIM APIs][1])

### Replace your single-rail NCCL line with dual-rail + QPs

```bash
ENV="$ENV -e NCCL_IB_HCA='$NCCL_HCA'"
ENV="$ENV -e NCCL_IB_QPS_PER_CONNECTION=4"
```

(`=` exact-match and `:port` are NCCL-supported.) ([NVIDIA Docs][2])

### Keep bootstrap consistent

These are fine *as long as* `OOB_IF` matches the IPs you hand to Ray:

```bash
ENV="$ENV -e NCCL_SOCKET_IFNAME=$OOB_IF"
ENV="$ENV -e GLOO_SOCKET_IFNAME=$OOB_IF"
```

### Optional: delete this unless you know you need it

```bash
ENV="$ENV -e UCX_NET_DEVICES=$OOB_IF"
```

NVIDIA sets `UCX_NET_DEVICES` in their MPI-based NCCL test playbook. vLLM+Ray doesn’t usually need UCX at all; leaving it wrong can only restrict things. ([NVIDIA NIM APIs][5])

---

## Sanity checks you should add (fast, prevents self-sabotage)

Before starting containers, fail fast if the expected rails aren’t up:

```bash
echo "=== Verify active rails on head ==="
ssh spark-2 "ibdev2netdev | egrep '(rocep1s0${PORT}|roceP2p1s0${PORT}).*Up' || true"

echo "=== Verify active rails on worker ==="
ssh spark-3 "ibdev2netdev | egrep '(rocep1s0${PORT}|roceP2p1s0${PORT}).*Up' || true"
```

NVIDIA’s own playbooks use `ibdev2netdev` to identify which port is Up. ([NVIDIA NIM APIs][1])

---

## What “success” should look like (numbers)

Rough mental math:

* 100 Gbps ≈ 12.5 GB/s (theoretical)
* 200 Gbps ≈ 25 GB/s (theoretical)

On Spark, people report ~26–27 GB/s “bus bandwidth” in NCCL tests when things are aligned right. ([NVIDIA Developer Forums][3])
If you’re stuck around ~12–13 GB/s class numbers, you’re basically single-rail.

---

If you want, I can also rewrite your script to **auto-detect whether f0 or f1 is Up** (via `ibdev2netdev` on spark‑2) and set `PORT/OOB_IF/HEAD_IP/NCCL_HCA` automatically—removes a whole class of “wrong port” bugs.

[1]: https://build.nvidia.com/spark/connect-two-sparks/stacked-sparks "Connect Two Sparks | DGX Spark"
[2]: https://docs.nvidia.com/deeplearning/nccl/archives/nccl_2203/user-guide/docs/env.html?utm_source=chatgpt.com "Environment Variables — NCCL 2.20.3 documentation"
[3]: https://forums.developer.nvidia.com/t/connectx-7-nic-in-dgx-spark/350417?page=2 "ConnectX-7 NIC in DGX Spark - Page 2 - DGX Spark / GB10 - NVIDIA Developer Forums"
[4]: https://developer.nvidia.com/blog/networking-reliability-and-observability-at-scale-with-nccl-2-24/?utm_source=chatgpt.com "Networking Reliability and Observability at Scale with ..."
[5]: https://build.nvidia.com/spark/nccl/stacked-sparks "NCCL for Two Sparks | DGX Spark"




**TLDR**
Done. I rewrote your script so it **auto-detects the connected Spark port group**, **forces dual-rail RoCE RDMA** (no bonding), and **keeps Ray/Gloo/NCCL bootstrap on the *same* interface/IP** (fixing your current f1-vs-192.168.100.x mismatch). It hard-fails if the 2nd rail has no IPv4 (because “dual-rail RoCE” is fake without GIDs/IPs on both rails).

[Download the rewritten script](sandbox:/mnt/data/start-vllm-multinode-dualrail.sh)

---

## What changed (the important bits)

* **Autodetects** whether `f0` or `f1` is the live direct link by checking:

  * both rails are `operstate=up` on **both nodes**
  * both rails have **IPv4** on **both nodes**
  * both rails can **ping via the correct interface** (`ping -I <iface> …`)
* Picks a consistent control plane:

  * `OOB_IF` becomes the *actual* linked `enp...` interface
  * `HEAD_IP` / `WORKER_IP` are read from `ip addr` on that interface (no hardcoded wrong subnet)
* Forces **dual-rail RDMA**:

  * `NCCL_IB_HCA="=rocep...:1,roceP2p...:1"`
  * `NCCL_IB_QPS_PER_CONNECTION=4` (default; overrideable)
* Removes the “UCX_NET_DEVICES=$OOB_IF” footgun (it can restrict/derail multi-rail behavior in weird ways).

---

## The rewritten script

```bash
#!/usr/bin/env bash
# Start vLLM multi-node on two DGX Sparks (head + worker) using dual-rail RoCE (RDMA) without bonding.
#
# What this script does differently vs the usual "single NIC" recipes:
#   - Autodetects which Spark NIC port group is actually linked between nodes (f0 or f1)
#   - Requires BOTH rails of that group to have IPv4 addresses on BOTH nodes (RoCE needs GIDs)
#   - Forces NCCL to use BOTH RDMA devices (dual-rail) via NCCL_IB_HCA
#   - Keeps Ray/Gloo/NCCL bootstrap on the same control-plane interface/IP (no mismatched subnets)
#
# Usage:
#   ./start-vllm-multinode-dualrail.sh [--debug] [--port f0|f1] [--iface <enp...>] [--dry-run]
#
# Environment overrides (optional):
#   HEAD_HOST, WORKER_HOST, MODEL, CONTAINER, VLLM_PORT, RAY_PORT,
#   HF_CACHE_DIR, SITE_CUSTOMIZE, DEBUG_NCCL, NCCL_IB_QPS_PER_CONNECTION, HF_HUB_OFFLINE
#
set -Eeuo pipefail

# ----------------------------
# Config (override via env)
# ----------------------------
HEAD_HOST=${HEAD_HOST:-spark-2}
WORKER_HOST=${WORKER_HOST:-spark-3}

MODEL=${MODEL:-QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ}
CONTAINER=${CONTAINER:-nvcr.io/nvidia/vllm:25.11-py3}

RAY_PORT=${RAY_PORT:-6379}
VLLM_PORT=${VLLM_PORT:-8000}

HF_CACHE_DIR=${HF_CACHE_DIR:-/home/tom/.cache/huggingface}
SITE_CUSTOMIZE=${SITE_CUSTOMIZE:-/home/tom/llm/sitecustomize.py}

DEBUG_NCCL=${DEBUG_NCCL:-0}
DRY_RUN=0

# Force selection (optional)
FORCE_PORT=${FORCE_PORT:-""}      # f0 or f1
FORCE_OOB_IF=${FORCE_OOB_IF:-""}  # e.g. enp1s0f0np0 or enp1s0f1np1

# NCCL tuning knobs (reasonable defaults)
NCCL_IB_QPS_PER_CONNECTION=${NCCL_IB_QPS_PER_CONNECTION:-4}
HF_HUB_OFFLINE=${HF_HUB_OFFLINE:-1}

# ----------------------------
# Arg parsing
# ----------------------------
usage() {
  cat <<USAGE
Usage: $0 [--debug] [--port f0|f1] [--iface <enp...>] [--dry-run]

  --debug           Enable NCCL debug logging inside containers
  --port f0|f1      Force using a specific Spark port group (skips autodetect)
  --iface <enp...>  Force control-plane interface (enp1s0f0np0 or enp1s0f1np1)
  --dry-run         Print what would run, do not execute

Examples:
  $0
  $0 --debug
  $0 --port f0
  $0 --iface enp1s0f1np1
USAGE
}

while [[ $# -gt 0 ]]; do
  case "$1" in
    --debug)
      DEBUG_NCCL=1
      shift
      ;;
    --dry-run)
      DRY_RUN=1
      shift
      ;;
    --port)
      FORCE_PORT="${2:-}"; shift 2
      ;;
    --iface)
      FORCE_OOB_IF="${2:-}"; shift 2
      ;;
    -h|--help)
      usage; exit 0
      ;;
    *)
      echo "ERROR: Unknown argument: $1" >&2
      usage
      exit 2
      ;;
  esac
done

# ----------------------------
# Helpers
# ----------------------------
log() { echo "[$(date +'%F %T')] $*"; }

die() {
  echo "ERROR: $*" >&2
  exit 1
}

ssh_run() {
  local host="$1"; shift
  local cmd="$*"
  if [[ "$DRY_RUN" == "1" ]]; then
    echo "+ ssh ${host} \"${cmd}\""
    return 0
  fi
  ssh -o BatchMode=yes "${host}" "${cmd}"
}

get_ipv4() {
  local host="$1" dev="$2"
  ssh_run "$host" "ip -4 -o addr show dev ${dev} 2>/dev/null | awk '{print \$4}' | cut -d/ -f1 | head -n1"
}

operstate() {
  local host="$1" dev="$2"
  ssh_run "$host" "cat /sys/class/net/${dev}/operstate 2>/dev/null || echo unknown"
}

require_cmd_on_node() {
  local host="$1" cmd="$2"
  ssh_run "$host" "command -v ${cmd} >/dev/null 2>&1" \
    || die "${cmd} not found on ${host}. Install rdma-core (for ibdev2netdev) and iproute2."
}

ping_once_from_head_iface() {
  local iface="$1" dst_ip="$2"
  # -I forces the egress interface so we actually validate each rail.
  ssh_run "$HEAD_HOST" "ping -I ${iface} -c 1 -W 1 ${dst_ip} >/dev/null 2>&1"
}

# ----------------------------
# Spark port group definitions
# ----------------------------
# Each Spark QSFP port group effectively has two rails:
#   - primary:  enp...
#   - sibling:  enP2p...
# and matching RDMA device names:
#   - primary:  rocep...
#   - sibling:  roceP2p...
#
# f0 group
P0_IF_A="enp1s0f0np0"
P0_IF_B="enP2p1s0f0np0"
P0_RDMA_A="rocep1s0f0"
P0_RDMA_B="roceP2p1s0f0"

# f1 group
P1_IF_A="enp1s0f1np1"
P1_IF_B="enP2p1s0f1np1"
P1_RDMA_A="rocep1s0f1"
P1_RDMA_B="roceP2p1s0f1"

# ----------------------------
# Port selection / validation
# ----------------------------
select_group() {
  local port="$1"
  case "$port" in
    f0)
      OOB_IF="$P0_IF_A"; RAIL_IF_A="$P0_IF_A"; RAIL_IF_B="$P0_IF_B"; RDMA_A="$P0_RDMA_A"; RDMA_B="$P0_RDMA_B" ;;
    f1)
      OOB_IF="$P1_IF_A"; RAIL_IF_A="$P1_IF_A"; RAIL_IF_B="$P1_IF_B"; RDMA_A="$P1_RDMA_A"; RDMA_B="$P1_RDMA_B" ;;
    *)
      die "Invalid port group '$port' (expected f0 or f1)" ;;
  esac
}

validate_group_on_node() {
  local host="$1"
  local st_a st_b ip_a ip_b

  st_a="$(operstate "$host" "$RAIL_IF_A" | tr -d '\r')"
  st_b="$(operstate "$host" "$RAIL_IF_B" | tr -d '\r')"

  ip_a="$(get_ipv4 "$host" "$RAIL_IF_A" | tr -d '\r')"
  ip_b="$(get_ipv4 "$host" "$RAIL_IF_B" | tr -d '\r')"

  echo "$st_a|$ip_a|$st_b|$ip_b"
}

pick_best_group() {
  local candidates=(f0 f1)

  # Forced by iface?
  if [[ -n "$FORCE_OOB_IF" ]]; then
    if [[ "$FORCE_OOB_IF" == "$P0_IF_A" ]]; then
      FORCE_PORT="f0"
    elif [[ "$FORCE_OOB_IF" == "$P1_IF_A" ]]; then
      FORCE_PORT="f1"
    else
      die "--iface must be ${P0_IF_A} or ${P1_IF_A} (got: $FORCE_OOB_IF)"
    fi
  fi

  # Forced by port?
  if [[ -n "$FORCE_PORT" ]]; then
    candidates=("$FORCE_PORT")
  fi

  for p in "${candidates[@]}"; do
    select_group "$p"

    local head_info worker_info
    head_info="$(validate_group_on_node "$HEAD_HOST")"
    worker_info="$(validate_group_on_node "$WORKER_HOST")"

    IFS='|' read -r head_st_a head_ip_a head_st_b head_ip_b <<<"$head_info"
    IFS='|' read -r work_st_a work_ip_a work_st_b work_ip_b <<<"$worker_info"

    # Require both rails have carrier and IPv4 on both nodes
    if [[ "$head_st_a" != "up" || "$head_st_b" != "up" || "$work_st_a" != "up" || "$work_st_b" != "up" ]]; then
      log "Skip $p: operstate not up on both rails (head: $RAIL_IF_A=$head_st_a $RAIL_IF_B=$head_st_b, worker: $RAIL_IF_A=$work_st_a $RAIL_IF_B=$work_st_b)"
      continue
    fi
    if [[ -z "$head_ip_a" || -z "$head_ip_b" || -z "$work_ip_a" || -z "$work_ip_b" ]]; then
      log "Skip $p: missing IPv4 on one/both rails (head: $RAIL_IF_A=$head_ip_a $RAIL_IF_B=$head_ip_b, worker: $RAIL_IF_A=$work_ip_a $RAIL_IF_B=$work_ip_b)"
      continue
    fi

    # Quick connectivity check on BOTH rails (forces egress interface)
    if ! ping_once_from_head_iface "$RAIL_IF_A" "$work_ip_a"; then
      log "Skip $p: ping head->worker failed on rail A ($RAIL_IF_A -> $work_ip_a)"
      continue
    fi
    if ! ping_once_from_head_iface "$RAIL_IF_B" "$work_ip_b"; then
      log "Skip $p: ping head->worker failed on rail B ($RAIL_IF_B -> $work_ip_b)"
      continue
    fi

    # Selected
    PORT_GROUP="$p"
    HEAD_IP="$head_ip_a"
    WORKER_IP="$work_ip_a"
    HEAD_RAIL_IP_B="$head_ip_b"
    WORKER_RAIL_IP_B="$work_ip_b"
    return 0
  done

  die "Could not find a usable dual-rail link between ${HEAD_HOST} and ${WORKER_HOST}.
- Ensure the QSFP cable is connected between the same port group on both boxes.
- Ensure BOTH rails for that group have IPv4 addresses on BOTH nodes (RoCE needs them).
- Run: ssh ${HEAD_HOST} ibdev2netdev ; ssh ${WORKER_HOST} ibdev2netdev"
}

# ----------------------------
# Main
# ----------------------------
log "Checking prerequisites..."
require_cmd_on_node "$HEAD_HOST" ibdev2netdev
require_cmd_on_node "$WORKER_HOST" ibdev2netdev
require_cmd_on_node "$HEAD_HOST" docker
require_cmd_on_node "$WORKER_HOST" docker

log "Autodetecting linked Spark port group (dual-rail)..."
pick_best_group

log "Selected port group: ${PORT_GROUP}"
log "Control-plane: ${OOB_IF}"
log "Head IP:      ${HEAD_IP}"
log "Worker IP:    ${WORKER_IP}"
log "Head rail #2: ${RAIL_IF_B} = ${HEAD_RAIL_IP_B}"
log "Worker rail#2:${RAIL_IF_B} = ${WORKER_RAIL_IP_B}"

# NCCL: force dual-rail RDMA
NCCL_HCA="=${RDMA_A}:1,${RDMA_B}:1"

log "NCCL_IB_HCA=${NCCL_HCA}"
log "NCCL_IB_QPS_PER_CONNECTION=${NCCL_IB_QPS_PER_CONNECTION}"

# Common env
ENV_COMMON=(
  -e NCCL_NET_PLUGIN=none
  -e NCCL_DMABUF_ENABLE=0
  -e NCCL_NET_GDR_LEVEL=LOC
  -e NCCL_NET_GDR_C2C=0
  -e NCCL_IB_HCA="$NCCL_HCA"
  -e NCCL_IB_QPS_PER_CONNECTION="$NCCL_IB_QPS_PER_CONNECTION"
  -e NCCL_SOCKET_IFNAME="$OOB_IF"
  -e GLOO_SOCKET_IFNAME="$OOB_IF"
  -e RAY_memory_monitor_refresh_ms=0
  -e HF_HUB_OFFLINE="$HF_HUB_OFFLINE"
)

if [[ "$DEBUG_NCCL" == "1" ]]; then
  ENV_COMMON+=(
    -e NCCL_DEBUG=INFO
    -e NCCL_DEBUG_SUBSYS=INIT,NET
    -e NCCL_DEBUG_FILE=/tmp/nccl.%h.%p.log
  )
  log "NCCL debug enabled (container logs under /tmp/nccl.*.log)"
fi

ENV_HEAD=("${ENV_COMMON[@]}" -e VLLM_HOST_IP="$HEAD_IP")
ENV_WORKER=("${ENV_COMMON[@]}" -e VLLM_HOST_IP="$WORKER_IP")

# Volumes
VOLS=(
  -v "${HF_CACHE_DIR}:/root/.cache/huggingface"
)
if [[ -f "$SITE_CUSTOMIZE" ]]; then
  VOLS+=( -v "${SITE_CUSTOMIZE}:/usr/lib/python3.12/sitecustomize.py:ro" )
fi

RDMA_FLAGS=(--device=/dev/infiniband --ulimit memlock=-1 --cap-add=IPC_LOCK)

log "=== Cleanup ==="
ssh_run "$HEAD_HOST" "docker rm -f vllm-head 2>/dev/null || true"
ssh_run "$WORKER_HOST" "docker rm -f vllm-worker 2>/dev/null || true"

log "=== Debug: ibdev2netdev (Up links) ==="
ssh_run "$HEAD_HOST" "ibdev2netdev | egrep '\(Up\)' || true"
ssh_run "$WORKER_HOST" "ibdev2netdev | egrep '\(Up\)' || true"

log "=== Starting Ray head on ${HEAD_HOST} ==="
# Note: we deliberately pin Ray node IP to the control-plane IP we selected.
ssh_run "$HEAD_HOST" "docker run -d --name vllm-head --gpus all --shm-size 16g \
  --network host --ipc host \
  ${RDMA_FLAGS[*]} \
  ${ENV_HEAD[*]} \
  ${VOLS[*]} \
  ${CONTAINER} \
  ray start --head --port=${RAY_PORT} --node-ip-address=${HEAD_IP} --block"

log "Waiting for Ray head..."
if [[ "$DRY_RUN" != "1" ]]; then
  sleep 10
fi

log "=== Starting Ray worker on ${WORKER_HOST} ==="
ssh_run "$WORKER_HOST" "docker run -d --name vllm-worker --gpus all --shm-size 16g \
  --network host --ipc host \
  ${RDMA_FLAGS[*]} \
  ${ENV_WORKER[*]} \
  ${VOLS[*]} \
  ${CONTAINER} \
  ray start --address=${HEAD_IP}:${RAY_PORT} --node-ip-address=${WORKER_IP} --block"

log "Waiting for worker to join..."
if [[ "$DRY_RUN" != "1" ]]; then
  sleep 5
fi

log "=== Starting vLLM server on ${HEAD_HOST} ==="
VLLM_ARGS=(
  --tensor-parallel-size 2
  --trust-remote-code
  --enforce-eager
  --quantization awq
  --gpu-memory-utilization 0.75
  --kv-cache-dtype fp8
  --limit-mm-per-prompt.video 0
  --host 0.0.0.0
  --port "${VLLM_PORT}"
)

# vLLM forces VLLM_USE_RAY_COMPILED_DAG=1 for multi-node; keep default.

ssh_run "$HEAD_HOST" "docker exec -d \
  -e RAY_ADDRESS=${HEAD_IP}:${RAY_PORT} \
  -e VLLM_ATTENTION_BACKEND=TRITON_ATTN \
  vllm-head \
  vllm serve ${MODEL} ${VLLM_ARGS[*]}"

log "=== Done ==="
log "Head container:   ssh ${HEAD_HOST} 'docker logs -f vllm-head'"
log "Worker container: ssh ${WORKER_HOST} 'docker logs -f vllm-worker'"
log "NCCL logs (if --debug): inside containers under /tmp/nccl.*.log"
log "API (direct-link IP): http://${HEAD_IP}:${VLLM_PORT}/v1/chat/completions"
```

---

## How to run it

* Basic:

  ```bash
  ./start-vllm-multinode-dualrail.sh
  ```
* Force a specific port group if autodetect picks the “wrong” one (e.g., you have both ports cabled):

  ```bash
  ./start-vllm-multinode-dualrail.sh --port f0
  ```
* Debug NCCL:

  ```bash
  ./start-vllm-multinode-dualrail.sh --debug
  ```
* See commands without executing:

  ```bash
  ./start-vllm-multinode-dualrail.sh --dry-run
  ```

If this script errors with “missing IPv4 on one/both rails”, that’s not a script problem: you literally don’t have dual-rail RoCE configured yet (you need IPs on **both** `enp...` and `enP2p...` on both boxes).

**TLDR**

* Your **dual‑rail RoCE setup is working**. The NCCL log proves it’s using **both** `rocep1s0f1` and `roceP2p1s0f1`, and traffic is striped across **NET/IB/0** and **NET/IB/1**.
* The two real problems I see are: **(1)** you launch `vllm serve` via `docker exec -d`, which **hides logs and can silently fail**; **(2)** your benchmark runs are getting **SIGKILL/exit 137**, which in your tooling is very plausibly **timeout-kill**, not necessarily “OOM”.
* Next steps to make this repeatable: **log `vllm serve` output to a file + verify it’s alive**, then **measure the actual comm gain** with a quick NCCL bandwidth test and a controlled vLLM throughput test.

---

## 1) Dual‑rail RDMA: confirmed (you’re good here)

This line is the smoking gun:

* `NCCL INFO NET/IB : Using [0]rocep1s0f1:1/RoCE [1]roceP2p1s0f1:1/RoCE`

And then the channel mapping alternates between `NET/IB/0` and `NET/IB/1`, which is exactly what you want for dual‑rail striping.

Also:

* `GPU Direct RDMA Disabled…` is expected on Spark in many configs; it doesn’t mean “not RDMA”, it means “not GPUDirect RDMA”.

So: **the “dual rail while keeping RDMA” requirement is met.**

---

## 2) Your script’s weak link: `docker exec -d` makes failures invisible

You already hit this: Ray was up, but **`vllm serve` wasn’t running**, and because it was started detached you had **no stdout/stderr**.

### Fix: start vLLM with log redirection + a health check

Replace your detached exec with something like:

```bash
ssh_run "$HEAD_HOST" "docker exec vllm-head bash -lc '
  export RAY_ADDRESS=$HEAD_IP:$RAY_PORT
  export VLLM_ATTENTION_BACKEND=TRITON_ATTN
  nohup vllm serve $MODEL $VLLM_ARGS > /tmp/vllm-serve.log 2>&1 &
  sleep 1
  pgrep -f \"vllm serve\" >/dev/null || { echo \"vllm failed, tail:\"; tail -200 /tmp/vllm-serve.log; exit 1; }
'"
```

Then your readiness check becomes deterministic and **local to the head** (no “wrong interface/IP” confusion):

```bash
ssh_run "$HEAD_HOST" "docker exec vllm-head bash -lc '
  for i in \$(seq 1 120); do
    curl -sf http://127.0.0.1:$VLLM_PORT/v1/models >/dev/null && exit 0
    sleep 2
  done
  echo \"vLLM not ready, tail:\"; tail -200 /tmp/vllm-serve.log; exit 1
'"
```

If you do only one thing: **do this**. It eliminates 80% of the “it’s running but not serving” ambiguity.

---

## 3) The exit code 137: don’t guess — classify it

Exit **137 = SIGKILL**. In your pasted trace, it’s happening to long background steps (“wait 10 min …”) and larger benchmark runs. That pattern screams **tooling timeout kill** as much as it screams OOM.

### Quick ways to tell which it is

**If it’s OOM kill (kernel):**

```bash
dmesg -T | grep -i -E "oom|killed process" | tail -50
```

**If it’s Docker memory limit kill:**

```bash
docker inspect vllm-head --format '{{json .HostConfig.Memory}}'
docker inspect vllm-worker --format '{{json .HostConfig.Memory}}'
```

**If it’s just your “agent runner” timing out:**
You’ll see kills correlate with your tool’s time budgets (your log literally shows `timeout: 10m 0s`, `timeout: 11m 0s`), and `dmesg` will be clean.

**My bet from your snippet:** it’s predominantly **tool timeouts**, because even a simple `sleep 600` got 137’d.

---

## 4) You may be benchmarking against the “wrong” IP (doesn’t break TP, but can confuse results)

Your script advertises the API at:

* `http://192.168.100.10:8000/...`

But you were curling:

* `http://192.168.102.11:8000/...`

It worked because the server binds `0.0.0.0`, so it’s reachable on multiple interfaces. That’s fine, *but*:

* If your benchmark client is not local, you might accidentally test via a **different network path** than you think.
* The **important** network path for TP is **head↔worker**, not client↔head — but still, don’t add noise.

Best practice for clean benchmarking:

* Run the load generator **on the head** and hit `127.0.0.1:$VLLM_PORT` (client network becomes irrelevant).
* Or hit `HEAD_IP` explicitly.

---

## 5) Make the performance win measurable (not vibes)

You set up dual‑rail. Now prove it did something.

### A) NCCL comm sanity check (expect roughly 2×)

Run `all_reduce_perf` across the two nodes (TP=2 equivalent comm patterns). What you’re looking for:

* “single-rail effective” tends to look like ~**11–13 GiB/s class** on a 100G-ish pipe
* “dual-rail effective” should push toward ~**20–23 GiB/s class** (realistic payload + overhead dependent)

If you don’t have nccl-tests handy, at least run a **single controlled** vLLM throughput test with identical settings and compare output tok/s before/after dual-rail.

### B) vLLM throughput test: keep it tight

Your “sweep” benchmarks (images, multiple concurrencies, resolutions) are extremely noisy and also long-running → timeout bait.

Do this instead:

* fix 1–2 prompt sizes (P50, P95)
* fix concurrency (e.g., 8, 32)
* fixed output length
* 3 runs, report median

That will actually tell you if dual‑rail moved the needle.

---

## 6) One tuning knob you’re missing (often matters for saturating both rails)

Right now you’re only setting `NCCL_IB_HCA`. Consider adding:

```bash
-e NCCL_IB_QPS_PER_CONNECTION=4
```

Reason: it increases parallelism per connection and often helps saturate multi-rail on platforms like Spark where you’re effectively combining two 100G-ish paths.

---

## 7) What I would do next (in order)

1. Patch the script to run `vllm serve` with `nohup … > /tmp/vllm-serve.log` and verify the PID exists.
2. Run a **single, short** vLLM throughput benchmark from the head to `127.0.0.1`.
3. Compare **dual‑rail vs forced single‑rail** by temporarily setting:

   * dual: `NCCL_IB_HCA='=rocep1s0f1:1,roceP2p1s0f1:1'`
   * single: `NCCL_IB_HCA='=rocep1s0f1:1'`
4. Only after that, do the big sweep. Otherwise you’ll just burn time inside a kill-prone harness.

---

If you paste:

* the exact command line your benchmark uses (especially concurrency + image sizes), and
* where the benchmark is running (head vs laptop vs some other host),

I can tell you whether exit 137 is *definitely* timeout vs OOM, and what the smallest benchmark that still reflects “real load” should be for Spark.

**TLDR**
On Spark‑2 you’re basically **CPU‑saturating the whole box**: `VLLM::EngineCore` is eating **~1655% CPU (~16.6 cores)** on a DGX Spark that only has **20 Arm cores total**. ([NVIDIA Docs][1]) That’s why load average is ~18 and you’re seeing **~60% “system” CPU**. The main driver is **vLLM’s EngineCore + Ray distributed execution** (often **tight polling / busy loops**, especially with **Ray compiled DAG**, which vLLM V1 enables by default under Ray). ([vLLM][2])
If you’re seeing this **even when idle**, the blunt fix is **`VLLM_SLEEP_WHEN_IDLE=1`** (drops idle CPU hard, at the cost of a small wake-up latency). ([vLLM][3])

---

## Why Spark‑2 is hot (and Spark‑3 isn’t)

You already spotted the important clue in `top`:

* **`VLLM::EngineCore` ≈ 1655% CPU** → ~16.55 cores pegged
* **`ray::RayWorkerWrapper` ≈ 90% CPU** → ~1 core
* overall **~8.5% idle**, **~60.7% system** time

That points to **one process burning a ton of cycles**, plus a lot of kernel time.

Spark‑2 is the **head**. In a Ray + vLLM TP=2 setup, the head typically does:

* scheduling / KV cache bookkeeping / request queueing (EngineCore),
* tokenizer + request parsing,
* multimodal preprocessing (image decode/resize/packing) when you’re serving VL.

So the *“why”* is not mysterious: the head isn’t just a coordinator — it’s doing real work.

But the *“why so much”* (16+ cores) is usually **polling behavior**, not “tokenizer is heavy”.

---

## The smoking gun: vLLM’s EngineCore is designed to poll

vLLM’s EngineCore has explicit code paths where it can enter a **tight polling loop** and only yields briefly (e.g. a `time.sleep(0.001)` guard) so background threads can make progress. ([vLLM][4])
That design is performance-friendly (low latency), but it can look like “why is this thing burning cores”.

Separately, vLLM’s Ray executor can use **Ray Compiled DAG** (previously aDAG) to reduce control-plane overhead; and vLLM’s env var docs state this is **set to 1 by default in V1 when Ray distributed executor is used**. ([vLLM][2])
There are real-world reports of **multiple CPU cores pegged at 100% even after a single inference** in Ray distributed setups. ([GitHub][5])

So: your observed “EngineCore eats 16 cores” is consistent with **busy polling / compiled-DAG/channel plumbing**, not necessarily “actual inference CPU”.

---

## Quick fork: is it busy when idle or busy because you’re actually doing work?

Don’t guess — check.

### If you’re idle (no incoming requests) and it’s still pegged

That’s the classic “spin when idle” behavior.

Fix:

* Add to the container env:

  ```bash
  -e VLLM_SLEEP_WHEN_IDLE=1
  ```

  This is explicitly documented as “reduce CPU usage when vLLM is idle” with a small latency penalty when requests resume. ([vLLM][3])

This is the cleanest first move because it doesn’t change correctness or topology.

### If you’re under real load (benchmarks, lots of images)

Then high CPU on the head can be legitimate:

* multimodal preprocessing is CPU-heavy,
* tokenization / batching logic runs on CPU,
* Ray overhead is non-trivial.

In that case, the “fix” is mostly **move the load generator off the head** (so CPU is reserved for serving), and/or reduce preprocessing cost (pre-resize inputs, fewer images per request, etc.).

---

## Why this hurts more on DGX Spark than on server boxes

DGX Spark has **20 cores total**. ([NVIDIA Docs][1])
So if EngineCore burns ~16.6 cores, you’ve got ~3–4 cores left for:

* the OS,
* networking,
* Ray head services,
* HTTP server threads,
* anything else you’re doing (bench client, monitoring, etc.).

On a 64–128 core server, the same “spin/poll” behavior is annoying but tolerable. On Spark, it can become a real throughput + tail-latency limiter.

Popular misconception: “GPU inference is GPU-bound; CPU doesn’t matter.”
On Spark: **CPU absolutely matters** because the CPU budget is small and vLLM’s control plane can be aggressive.

---

## What I would do (minimal changes first)

### 1) Enable idle sleep (if you care about idle CPU)

In your docker run for both head + worker:

```bash
-e VLLM_SLEEP_WHEN_IDLE=1
```

([vLLM][3])

### 2) Measure where the CPU is actually going (30 seconds, no drama)

On spark‑2 host:

```bash
pidstat -t -p $(pgrep -f 'VLLM::E|EngineCore|vllm serve' | tr '\n' ',' | sed 's/,$//') 1 10
mpstat -P ALL 1 10
```

If it’s polling, you’ll see:

* lots of threads at ~100% (or high) with very little variability,
* CPU stays high even when no requests are served.

### 3) If it’s Ray compiled DAG related and you want to experiment

vLLM exposes:

* `VLLM_USE_RAY_COMPILED_DAG`
* `VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE` (`auto|nccl|shm`)
  ([vLLM][2])

You can try:

```bash
-e VLLM_USE_RAY_COMPILED_DAG=0
```

I’m not promising it’ll reduce CPU (sometimes it can increase overhead), but it’s a clean A/B lever, and vLLM explicitly documents this switch. ([vLLM][2])

---

## One more thing: your current config isn’t “dual rail”, so don’t blame dual rail

From your env dump you showed:

* `NCCL_IB_HCA==rocep1s0f1:1` → you’re pinned to **one** HCA/rail (the double `==` is just because the value begins with `=`).

So your current CPU spike is not caused by “extra rail overhead”. It’s happening in **single‑rail** mode too.

---

If you paste:

* whether you were actively running the image benchmark at the time of that `top`, and
* `pidstat -t -p <EngineCorePID> 1 5` output,

I can tell you in one look if it’s **idle spin** vs **real preprocessing/scheduling load**, and which knob will actually move the needle.

[1]: https://docs.nvidia.com/dgx/dgx-spark/hardware.html?utm_source=chatgpt.com "Hardware Overview — DGX Spark User Guide"
[2]: https://docs.vllm.ai/en/v0.9.1/configuration/env_vars.html "Environment Variables - vLLM"
[3]: https://docs.vllm.ai/en/stable/configuration/env_vars/ "Environment Variables - vLLM"
[4]: https://docs.vllm.ai/en/stable/api/vllm/v1/engine/core/ "core - vLLM"
[5]: https://github.com/vllm-project/vllm/issues/21231 "[Bug]: 100% cpu usage on 3 cores on every node when using ray distributed pipeline parallel · Issue #21231 · vllm-project/vllm · GitHub"

**TLDR**
Yes — we can do a **clean A/B image benchmark** that actually answers the question you care about: “is the vision encoder still weight‑sharded TP, and should we flip it to batch‑DP (`--mm-encoder-tp-mode data`) for better performance?” Default is **`weights`** (i.e., encoder is sharded across TP ranks). ([vLLM][1])
Run the exact same image sweep **twice**: once as‑is, then once with `--mm-encoder-tp-mode data` (and optionally `--skip-mm-profiling` if startup/profiling is flaky). Qwen3‑VL’s own vLLM recipe explicitly recommends `--mm-encoder-tp-mode data` for better performance because the encoder is small and TP communication overhead can dominate. ([vLLM][2])

---

## 0) Ground truth: what mode are we in *right now*?

If you’re not passing `--mm-encoder-tp-mode`, you are in **default `weights`** mode (encoder weights sharded across TP ranks). ([vLLM][1])

That’s “MM encoding in TP” in practical terms: it forces cross‑rank comm per encoder layer.

---

## 1) Benchmark run #1: baseline (current deployment)

Goal: capture a reproducible baseline for **req/s** and **img_tok/s** across resolution + concurrency.

### Run it from a client that won’t steal head CPU

If you have a third node (e.g. spark‑1), run the benchmark there. Otherwise run on the head but be aware you’re competing with EngineCore.

### Command pattern (use your existing script)

You previously ran something like:

```bash
python3 /home/tom/llm/benchmark_vllm.py --sweep -t 256
```

Do the same, but **log it** and **pin the URL** so you’re not accidentally hitting a different NIC:

```bash
TS=$(date +%F_%H%M%S)
export VLLM_BASE_URL="http://192.168.100.10:8000"   # adjust if your script uses a different var
python3 /home/tom/llm/benchmark_vllm.py --sweep -t 256 2>&1 | tee bench_mm_weights_${TS}.log
```

If your script doesn’t read `VLLM_BASE_URL`, just add whatever flag it supports (`--base-url`, `--host`, etc.). The point is: **keep it constant** across the A/B.

### Optional: collect CPU/GPU context so results are interpretable

On spark‑2 during the benchmark:

```bash
mpstat -P ALL 1 20
```

This tells you if you’re CPU‑bound (common on Spark heads).

---

## 2) Switch config: enable batch‑DP vision encoder

For Qwen3‑VL, vLLM explicitly says:

* `--mm-encoder-tp-mode data` runs the vision encoder in a **data-parallel fashion** for better performance (encoder is small; TP brings little gain and adds comm overhead)
* It **uses more memory**, so you may need to lower `--gpu-memory-utilization` ([vLLM][2])

### Change only ONE thing

Add to your `vllm serve` args:

```bash
--mm-encoder-tp-mode data
```

([vLLM][1])

If you’ve previously seen “hangy” startup behavior (profiling), add:

```bash
--skip-mm-profiling
```

This skips multimodal memory profiling to reduce init time (but puts memory safety on you). ([vLLM][1])

### Practical safe starting point

If you’re already near the memory edge, drop:

```bash
--gpu-memory-utilization 0.75
```

to e.g.

```bash
--gpu-memory-utilization 0.70
```

(because DP replicates encoder weights). vLLM’s Qwen3‑VL recipe warns you might need to adjust this. ([vLLM][2])

### Verify it actually took effect

Make sure your server logs include something indicating the encoder TP mode. If it silently falls back (model doesn’t support DP), vLLM can fall back to `weights`. ([vLLM][3])

---

## 3) Benchmark run #2: same sweep, same conditions

Run the exact same benchmark again:

```bash
TS=$(date +%F_%H%M%S)
export VLLM_BASE_URL="http://192.168.100.10:8000"
python3 /home/tom/llm/benchmark_vllm.py --sweep -t 256 2>&1 | tee bench_mm_data_${TS}.log
```

---

## 4) What you should expect (and what would falsify it)

### Expected if `data` is helping (common for VL models)

You should see:

* **higher req/s** and/or **higher img_tok/s** at **moderate+ concurrency** (e.g. 8/16/32),
* biggest gains at larger images (more encoder work),
* sometimes better stability (less comm contention).

This aligns with vLLM’s own rationale: encoder is small, TP gives little benefit but adds **all-reduce per layer**, so DP can win. ([vLLM][4])

### If you see no change

Likely causes:

* your batch rarely contains >1 image per step (DP can’t split),
* you’re **CPU‑bound** on preprocessing/scheduling (Spark head), not comm‑bound,
* vLLM fell back to `weights` (DP unsupported).

### If it gets worse

Also plausible:

* you’re memory‑pressure‑bound (more encoder replication reduces KV cache headroom),
* you’re in low concurrency (DP wastes a GPU on encoder work when there’s nothing to split).

---

## What I need from you after you run it

Paste the two benchmark tables (weights vs data) and I’ll compute:

* per-resolution speedups,
* where the crossover happens (what concurrency makes DP worth it),
* whether you’re CPU‑limited (based on the shape of the curves).

If you want the “no ambiguity” version: I’ll also give you a tiny parser that turns both logs into a CSV and prints the deltas.

[1]: https://docs.vllm.ai/en/stable/configuration/engine_args/ "Engine Arguments - vLLM"
[2]: https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html?utm_source=chatgpt.com "Qwen3-VL Usage Guide - vLLM Recipes"
[3]: https://docs.vllm.ai/en/latest/api/vllm/config/multimodal/?utm_source=chatgpt.com "vllm.config.multimodal"
[4]: https://docs.vllm.ai/en/stable/configuration/optimization/?utm_source=chatgpt.com "Optimization and Tuning - vLLM"

**TLDR**

* For **Qwen3‑VL‑235B‑A22B**, switching vLLM’s vision encoder to batch‑level DP (`--mm-encoder-tp-mode data`) likely costs only **~0.4 GB (~381 MiB) extra memory per Spark** in your **TP=2** setup (because the vision encoder is ~**0.4B params** ≈ **0.75 GiB** in FP16/BF16, and DP stops sharding it). ([rocm.blogs.amd.com][1])
* The **bigger RAM hog** is often vLLM’s **multimodal processor cache** (default **4 GiB**, duplicated by process count), not the encoder weights. ([vLLM][2])
* Best config on **2× DGX Spark** for your model: **TP=2 + `--mm-encoder-tp-mode data` + FP8 KV + sane max context (e.g., 32k)**, and only use **dual‑rail RoCE** if your benchmarks show a real win (Spark’s NIC throughput is often **PCIe‑limited**, so “2 rails = 2× perf” is not a safe assumption). ([vLLM][3])

---

## How much extra RAM does `--mm-encoder-tp-mode data` take?

### What the flag actually changes (first principles)

vLLM has two *encoder TP modes*: ([vLLM][2])

* **`weights` (default):** vision encoder weights are **sharded** across TP ranks (TP behavior).
* **`data`:** vision encoder weights are **replicated** on each TP rank, and the *batch* is split across ranks (batch-level DP), avoiding per-layer all-reduces during the encoder forward pass.

So the memory delta is basically:

[
\text{extra per rank} \approx \text{encoder_weights_size} \times (1 - \frac{1}{TP})
]

For **TP=2**, that’s **~50% of the encoder weights** *per rank*.

### Qwen3‑VL’s vision encoder size

Qwen’s own team confirmed the vision encoder is initialized from **SigLIP2 so400m** (“so400m” is the ~400M class). ([Hugging Face][4])
AMD’s writeup on vLLM DP‑vision also lists **Qwen3‑VL‑235B‑A22B-Instruct encoder size = 0.4B params**. ([rocm.blogs.amd.com][1])

### Convert that into bytes

Assuming the vision encoder weights are in **FP16/BF16** (2 bytes/param — typical for inference; AWQ mainly hits the LLM, not necessarily the vision tower):

* Encoder params: **0.4B**
* Bytes/param: **2**
* Total encoder weights:
  **0.4e9 × 2 = 0.8e9 bytes ≈ 0.75 GiB**

Now compare modes with **TP=2**:

| Mode                |     Encoder weights per rank |             Extra vs weights-mode |
| ------------------- | ---------------------------: | --------------------------------: |
| `weights` (sharded) | ~0.75 GiB / 2 = **0.37 GiB** |                          baseline |
| `data` (replicated) |                 **0.75 GiB** | **+0.37 GiB (~381 MiB, ~0.4 GB)** |

So on **two Sparks**, total extra memory across the cluster is ~**0.75 GiB (~0.8 GB)**, split as ~**0.37 GiB per node**.

### Reality check: what can dwarf this?

vLLM’s **mm processor cache** defaults to **4 GiB** and is duplicated across processes, with total memory roughly:

[
\text{mm_processor_cache_gb} \times (\text{api_server_count} + \text{data_parallel_size})
]

That can easily be **8 GiB** in common layouts—way bigger than the ~0.4 GB you’re sweating here. ([vLLM][2])

---

## Should you actually use mm-encoder DP on 2× Spark?

### Popular opinion

“Turn it on. The vision encoder is small; sharding it just adds communication. DP should be faster.”

That’s broadly consistent with how the feature is described and why it exists. ([vLLM][2])

### My conclusion (for *your* 2-node case)

**Yes, it’s usually the right move to try first**, because:

* Your encoder is **small (~0.4B)**, so TP sharding gives little compute benefit but adds cross-node sync. ([rocm.blogs.amd.com][1])
* You’re on **two nodes**, so every “all‑reduce per encoder layer” is **inter-node** (worst case).
* The memory cost is **sub‑GiB per node**, which is noise compared to Spark’s **128GB unified pool**. ([ServeTheHome][5])

But don’t expect miracles:

* AMD’s measurements for Qwen3‑VL show an average gain around **~6%** and “not highly consistent” (bigger wins at larger image sizes / more items). ([Hugging Face][6])
* On TP=2 with real inter-node comm, you *might* see more benefit than their setup—or you might not, depending on whether your bottleneck is actually the encoder or the vLLM/Ray orchestration.

---

## Best config on dual DGX Spark for your model (and generally)

I’m going to be blunt: on **2× Spark**, your best config is usually the one that **minimizes inter-node communication** and **keeps head CPU from becoming the throttle**, because Spark is a unified-memory, CPU+GPU SoC box and multi-node overhead shows up fast.

### 1) For your current model (Qwen3‑VL‑235B‑A22B‑Instruct‑AWQ) on 2 nodes

**Core flags I’d run:**

* `--tensor-parallel-size 2`
* `--mm-encoder-tp-mode data`  *(low risk, small memory hit, likely helps)*
* `--kv-cache-dtype fp8` *(you’re already doing this; keep it)*
* `--limit-mm-per-prompt.video 0` *(keep; video explodes tokens)*
* **Set `--max-model-len` explicitly** (ex: **32768**) unless you truly need huge context

  * Your AWQ model card examples use **32k**. ([Hugging Face][7])
* Consider `--async-scheduling` (vLLM’s Qwen3‑VL guide uses it heavily). ([vLLM][8])

**CPU sanity:**

* Set `OMP_NUM_THREADS=1` (vLLM Qwen3‑VL guide calls this out). ([vLLM][3])
  This doesn’t fix all Ray/vLLM overhead, but it avoids death-by-threading in preprocessing.

**RAM sanity (often overlooked):**

* If host RAM is getting crushed, reduce mm cache:

  * `--mm-processor-cache-gb 1` (or 2), not the default 4. ([vLLM][2])

### 2) Dual-rail vs single-rail on Spark: don’t assume dual wins

**Popular opinion:** dual-rail should be faster (2× links).
**Spark reality:** your NIC is a ConnectX‑7, but on Spark it’s often constrained by **PCIe Gen5 x4**, and people have measured **~96 Gbps total** even when using both ports with iperf. That strongly suggests the box can be *bus-limited* before you ever benefit from dual-rail. ([ServeTheHome][5])

So for Spark specifically:

* **Default recommendation:** **single-rail RoCE** for simplicity + fewer moving parts.
* **Enable dual-rail only if your benchmarks show improvement** (and your head CPU isn’t already saturated by EngineCore/Ray).

### 3) “Best in general” for 2× Spark (not model-specific)

If you’re choosing what to deploy:

* **Best latency/throughput per watt:** pick a model that fits on **one Spark** (avoid inter-node TP entirely).
* If you must go multi-node TP:

  * Prefer architectures that reduce comm (MoE can help *if* configured right)
  * Use encoder DP for multimodal towers
  * Keep context length conservative
  * Avoid “clever” networking unless proven

---

## What I would do next (pragmatic)

Run **two quick A/B benchmarks**:

1. Current setup but add:

* `--mm-encoder-tp-mode data`

2. Same as (1) but also shrink mm cache:

* `--mm-processor-cache-gb 1`

Keep everything else constant. If you don’t see at least a few percent on your image-heavy benchmark, then your bottleneck isn’t encoder comm—it’s elsewhere (often head CPU / Ray / unified memory behavior).

If you paste your exact vLLM command line + the image benchmark script you’re using (or its args), I can suggest the *specific* knobs that are most likely to move the needle on **Spark** rather than generic vLLM advice.

[1]: https://rocm.blogs.amd.com/software-tools-optimization/vllm-dp-vision/README.html?utm_source=chatgpt.com "The One-Line Optimization for Large Multimodal Models"
[2]: https://docs.vllm.ai/en/stable/configuration/engine_args/ "Engine Arguments - vLLM"
[3]: https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html "Qwen3-VL Usage Guide - vLLM Recipes"
[4]: https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct/discussions/1 "Qwen/Qwen3-VL-235B-A22B-Instruct · Is the Qwen3-VL Vision Encoder Based on siglip2-so400m-patch16?"
[5]: https://www.servethehome.com/nvidia-dgx-spark-review-the-gb10-machine-is-so-freaking-cool/2/ "NVIDIA DGX Spark Review The GB10 Machine is so Freaking Cool - Page 2 of 4 - ServeTheHome"
[6]: https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct/discussions "Qwen/Qwen3-VL-235B-A22B-Instruct · Discussions"
[7]: https://huggingface.co/QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ "QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ · Hugging Face"
[8]: https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html?utm_source=chatgpt.com "Qwen3-VL Usage Guide - vLLM Recipes"

**TLDR**

* In **vLLM**, there isn’t really a “mixed chunking” switch; the thing people usually mean is **chunked prefill** (aka *mixing prefill chunks with decode in the same batches*).
* For **Qwen 235B (MoE)** on **two DGX Spark** boxes, **yes, you generally want this behavior ON** for real serving (multiple users / variable prompt lengths). In **vLLM V1 it’s already enabled by default**; you mostly tune `--max-num-batched-tokens`. ([vLLM][1])
* **Only consider turning it OFF** if you’re doing **single-stream benchmarks** or ultra-low concurrency where you only care about the absolute minimum **TTFT** (time-to-first-token). Chunking adds some overhead and can slightly increase TTFT. ([Hugging Face][2])
* If by “chunking” you actually meant **Dual Chunk Attention (1M context)**: that’s a *different* feature with different flags, and it can be brittle across versions. ([Hugging Face][3])

---

## What “mixed chunking” maps to in vLLM

A lot of people pick up the term from **SGLang** (`--enable-mixed-chunk`) — that’s basically a scheduler strategy that **chunks long prefills and mixes them with decode** so long prompts don’t “freeze” everyone else.

In **vLLM**, the equivalent concept is **Chunked Prefill**:

* vLLM processes large prefills in **smaller chunks** and **batches them together with decode** requests. ([vLLM][1])
* In **vLLM V1**, this is **enabled by default whenever possible**, and the scheduler **prioritizes decode** first, then fills remaining token budget with prefill chunks. ([vLLM][1])

So: if you’re on V1, you’re probably already “doing mixed chunking” without realizing it.

---

## Should you enable it for Qwen 235B on dual DGX Spark?

### My conclusion (from first principles)

**Yes for production / multi-user serving.** Prefill is typically **compute-heavy**, decode is typically **memory-bandwidth bound**; mixing them is a straightforward way to keep the machine busier instead of alternating between “compute saturated” and “bandwidth saturated.” Chunking prevents one giant prefill from dominating the step time and stalling decode for everyone else. ([vLLM][1])

### The “popular opinion”

“Leave chunked prefill on; it’s a good default.” For example, a team article describing large-scale vLLM serving reported **~+50% total token throughput** improvement from chunked prefill in a standard deployment, and they enabled it everywhere. ([Hugging Face][2])

### When I would *not* bother

* **Single user, one request at a time, short prompts**: chunking doesn’t help much because there’s nothing else to overlap with; it can add overhead and slightly worsen TTFT. ([Hugging Face][2])
* **If you’re chasing a “best possible TTFT” leaderboard number** at QPS≈0: you might prefer contiguous prefill.

---

## The knob that actually matters: `--max-num-batched-tokens`

In vLLM V1, the practical control for how “aggressive” chunked prefill behaves is `max_num_batched_tokens`:

* **Smaller** (example given: **2048**) → **better ITL** (inter-token latency) because fewer prefills slow down decodes. ([vLLM][1])
* **Larger** → **better TTFT** because you can process more prompt tokens per batch. ([vLLM][1])
* vLLM docs explicitly recommend **`> 8192` for throughput** (especially on big GPUs / smaller models). ([vLLM][1])

### Practical starting points for *two-node* DGX Spark

Two-node adds more communication overhead per forward pass, so you generally don’t want absurdly tiny chunk sizes that explode the number of iterations.

I’d start like this (and then sweep):

* **Interactive chat (smooth streaming)**: `--max-num-batched-tokens 2048` or `4096`
* **Throughput / batch jobs**: `--max-num-batched-tokens 8192` or `16384`

Those numbers aren’t magic; they’re just aligned with vLLM’s own guidance about ITL vs TTFT vs throughput. ([vLLM][1])

---

## Dual DGX Spark reality check

Two DGX Spark systems are explicitly positioned to run larger models by linking them:

* Each DGX Spark has **128 GB coherent unified system memory**. ([NVIDIA][4])
* NVIDIA says **ConnectX networking can connect two DGX Spark systems** to work with models **up to 405B parameters**, and lists a **ConnectX-7 NIC @ 200 Gbps**. ([NVIDIA][4])

So, yes, **“Qwen 235B” is in the envelope** of what the dual system is meant to tackle *in principle*. But: chunked prefill won’t fix the hard parts (weight memory footprint, KV cache pressure, inter-node comm). It only helps scheduling/overlap.

Also note vLLM’s own warning: if you hit **KV-cache preemption**, you’ll see recompute and latency pain; vLLM suggests increasing `gpu_memory_utilization` or TP, or **decreasing `max_num_seqs` / `max_num_batched_tokens`** to reduce KV pressure. ([vLLM][1])

---

## If your real question is “Qwen 235B with 1M context”

If you mean the Qwen “1M context” / Dual Chunk Attention setup (common in Qwen model cards):

* You’ll be looking at **`VLLM_ATTENTION_BACKEND=DUAL_CHUNK_FLASH_ATTN`** plus **`--enforce-eager`** and **`--enable-chunked-prefill`**, and typically extremely low concurrency (`--max-num-seqs 1`). ([Hugging Face][3])
* This path has had version-sensitive issues (e.g., hangs/crashes reported for dual chunk attention in some vLLM commits). ([GitHub][5])

So: only go there if you *really* need ultra-long context. Otherwise it’s complexity and risk for little benefit.

---

## A “don’t guess, measure” micro-plan (fast sanity check)

If you want to be rigorous, run two serving benchmarks (same traffic):

1. **Chunked prefill effectively ON** (V1 default; pick `max_num_batched_tokens`)
2. **Make it behave like prefill-first** by setting `max_num_batched_tokens ≈ max_model_len` (vLLM notes this is close to the old behavior, though still decode-prioritized). ([vLLM][1])

Compare:

* **TTFT p50/p99**
* **ITL p50/p99**
* **Total tokens/sec**

Under real concurrency, chunked prefill usually wins on ITL and throughput; under QPS≈0, it’s often neutral or slightly worse on TTFT. ([Hugging Face][2])

---

### Bottom line

If you’re serving real traffic on dual DGX Spark: **don’t disable the vLLM equivalent of “mixed chunking.”** Keep **chunked prefill / mixed prefill+decode batching** and tune `--max-num-batched-tokens` to your latency-vs-throughput goal. ([vLLM][1])

[1]: https://docs.vllm.ai/en/stable/configuration/optimization/ "https://docs.vllm.ai/en/stable/configuration/optimization/"
[2]: https://huggingface.co/blog/tngtech/llm-performance-prefill-decode-concurrent-requests "https://huggingface.co/blog/tngtech/llm-performance-prefill-decode-concurrent-requests"
[3]: https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507 "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"
[4]: https://www.nvidia.com/en-us/products/workstations/dgx-spark/ "https://www.nvidia.com/en-us/products/workstations/dgx-spark/"
[5]: https://github.com/vllm-project/vllm/issues/20484 "https://github.com/vllm-project/vllm/issues/20484"


**TLDR**
I can’t literally edit your files from here, but here’s the exact patch I’d apply to your start script: cap CPU thread fan‑out (`OMP/MKL/BLAS/NUMEXPR`), ensure idle sleeping (`VLLM_SLEEP_WHEN_IDLE=1`), dedup Ray logs (`RAY_DEDUP_LOGS=1`), reduce multimodal processor cache (`--mm-processor-cache-gb 1`), and cut logging overhead (`--disable-log-stats`, `--disable-uvicorn-access-log`, `--uvicorn-log-level warning`). No redeploy/restart needed until you choose.

---

## 1) Patch for a “bare metal” start script (runs `vllm serve` directly)

Apply this pattern to your `start_spark2.sh` (or whatever it’s called):

```diff
diff --git a/start_spark2.sh b/start_spark2.sh
--- a/start_spark2.sh
+++ b/start_spark2.sh
@@ -1,8 +1,35 @@
 #!/usr/bin/env bash
 set -euo pipefail

+# -------------------------
+# CPU / log throttles (safe)
+# -------------------------
+# Cap common CPU thread pools (prevents surprise 32/64-thread fan-out)
+export OMP_NUM_THREADS="${OMP_NUM_THREADS:-1}"
+export MKL_NUM_THREADS="${MKL_NUM_THREADS:-1}"
+export OPENBLAS_NUM_THREADS="${OPENBLAS_NUM_THREADS:-1}"
+export NUMEXPR_NUM_THREADS="${NUMEXPR_NUM_THREADS:-1}"
+export TOKENIZERS_PARALLELISM="${TOKENIZERS_PARALLELISM:-false}"
+
+# vLLM: reduce CPU usage when idle (small latency penalty when a request arrives)
+export VLLM_SLEEP_WHEN_IDLE="${VLLM_SLEEP_WHEN_IDLE:-1}"
+
+# vLLM: quieter logs (less work formatting/printing INFO spam)
+export VLLM_LOGGING_LEVEL="${VLLM_LOGGING_LEVEL:-WARNING}"
+
+# Ray: deduplicate logs (cuts log churn)
+export RAY_DEDUP_LOGS="${RAY_DEDUP_LOGS:-1}"
+
 # ... your existing env/model settings here ...
 # MODEL=...
 # PORT=...

-vllm serve "$MODEL" --host 0.0.0.0 --port "$PORT" \
+vllm serve "$MODEL" --host 0.0.0.0 --port "$PORT" \
+  --mm-processor-cache-gb "${MM_PROCESSOR_CACHE_GB:-1}" \
+  --disable-log-stats \
+  --disable-uvicorn-access-log \
+  --uvicorn-log-level warning \
+  --disable-log-requests \
   ${VLLM_EXTRA_ARGS:-}
```

Why these exact flags/vars:

* `--mm-processor-cache-gb`: default is **4 GiB**; setting to **1 GiB** cuts that cache by **75%**, and vLLM notes it gets duplicated per API process and DP size. ([docs.vllm.ai][1])
* `VLLM_SLEEP_WHEN_IDLE=1` is explicitly documented to reduce CPU usage when vLLM is idle (with a small latency penalty when requests resume). ([docs.vllm.ai][2])
* `--disable-log-stats`, `--disable-uvicorn-access-log`, `--uvicorn-log-level warning` are all supported server args and reduce logging overhead. ([docs.vllm.ai][3])
* `--disable-log-requests` is marked **deprecated** in current docs (request logging is controlled by `--enable-log-requests`), but keeping it is a decent “belt-and-suspenders” move for older deployments where request logging used to be on by default. ([docs.vllm.ai][3])

---

## 2) Patch if your script starts vLLM via `docker run`

If your script does `docker run … vllm serve …`, you must pass env vars into the container:

```diff
 docker run --rm \
+  -e OMP_NUM_THREADS=1 \
+  -e MKL_NUM_THREADS=1 \
+  -e OPENBLAS_NUM_THREADS=1 \
+  -e NUMEXPR_NUM_THREADS=1 \
+  -e TOKENIZERS_PARALLELISM=false \
+  -e VLLM_SLEEP_WHEN_IDLE=1 \
+  -e VLLM_LOGGING_LEVEL=WARNING \
+  -e RAY_DEDUP_LOGS=1 \
   ...your existing mounts/ports/gpus... \
   your-vllm-image:tag \
   vllm serve "$MODEL" --host 0.0.0.0 --port "$PORT" \
+    --mm-processor-cache-gb 1 \
+    --disable-log-stats \
+    --disable-uvicorn-access-log \
+    --uvicorn-log-level warning \
+    --disable-log-requests \
     ${VLLM_EXTRA_ARGS:-}
```

Same rationale/citations as above. ([docs.vllm.ai][1])

---

## 3) Notes (skeptical take)

* **Reducing CPU doesn’t magically lower GPU power** unless the CPU churn is *preventing* the GPU from entering lower power states (e.g., constant polling / tiny kernels / log-induced synchronization). This patch is mostly about eliminating **idle CPU burn + log churn** and **multimodal preproc overhead**; GPU power might drop *only if* those were keeping the GPU “awake.”
* The **most likely win** for GPU power is still: fewer tokens/sec, fewer concurrent sequences, or better idle behavior. CPU tweaks are “supporting actors.”

---

If you paste your current start script here (or just the `docker run` / `vllm serve` line), I’ll return a *precise* diff against your exact file instead of this generic patch.

[1]: https://docs.vllm.ai/en/stable/configuration/engine_args/ "Engine Arguments - vLLM"
[2]: https://docs.vllm.ai/en/stable/configuration/env_vars/?utm_source=chatgpt.com "Environment Variables - vLLM"
[3]: https://docs.vllm.ai/en/stable/cli/serve/ "vllm serve - vLLM"


**TLDR**

You’re seeing ~**1k tok/s “encode”** because you’re running a **monster MoE VLM** (Qwen3‑VL‑235B‑A22B) on **hardware that’s fundamentally bandwidth + interconnect constrained** (DGX Spark’s **273 GB/s unified LPDDR5x**) and, if you’re doing **cross‑node TP**, you’re paying **communication + host‑staging** costs (Spark has **no GPUDirect RDMA**). On top of that, if you’re using `--enforce-eager`, you’re **turning off CUDA Graphs**, which is literally leaving performance on the table. Net: **1k tok/s prefill is not crazy**; it’s often the expected order of magnitude for this class of model even on much bigger GPUs, and Spark makes it worse.

---

## 1) First: “encode” is ambiguous (and people measure it wrong)

With vLLM there are *two* very different “encoding” paths:

### A) **Prefill / prompt encoding** (text + any “prompt tokens”)

The clean way to measure that on a running server is the Prometheus counter:

* `vllm:prompt_tokens_total` (prefill tokens processed) ([docs.vllm.ai][1])

Take a delta over time → prompt tok/s.

### B) **Vision encoding** (image → embeddings → “image tokens”)

This can dominate wall time when you attach images, especially at high resolution and high concurrency. vLLM’s Qwen3‑VL recipe explicitly calls out knobs like `--mm-encoder-tp-mode data` and multimodal caching because the vision side can be a real bottleneck. ([docs.vllm.ai][2])

So if your “encode tps” number is coming from an end‑to‑end benchmark that includes image preprocessing + vision encoder, **1k** can be completely normal.

---

## 2) Reality check: ~1k tok/s is the right *order of magnitude* for 235B-class models

Qwen’s own published speed benchmark (on **H20 96GB**) reports *end-to-end* speed defined as:

[
\text{Speed} = \frac{\text{prompt tokens} + \text{generation tokens}}{\text{time}}
]

with batch size 1. ([qwen.readthedocs.io][3])

For **Qwen3‑235B‑A22B** they report numbers in the **hundreds to ~1.6k tokens/s** range depending on context length + quantization + GPU count. ([qwen.readthedocs.io][3])

That’s on big datacenter GPUs with HBM. So expecting “5k–20k tok/s encode” for a 235B model is just not grounded.

---

## 3) The hard limit on DGX Spark: memory bandwidth (and it’s *not* HBM)

DGX Spark is **unified LPDDR5x**, not HBM, and NVIDIA lists:

* **128 GB LPDDR5x coherent unified memory**
* **273 GB/s bandwidth** ([NVIDIA][4])

That bandwidth is shared and is *the* obvious limiter for large-model inference (weights + activations + KV traffic). LMSYS’ in-depth writeup also flags **memory bandwidth as the key inference bottleneck** on Spark. ([LMSYS][5])

So even if your GPU “utilization” looks high, your **effective tokens/sec** can be capped by **memory traffic**, especially on large models and long contexts.

---

## 4) Cross-node TP on Spark is extra painful because **GPUDirect RDMA is not supported**

This is the big one for **dual-node** Spark setups:

NVIDIA explicitly states **GPUDirect RDMA is not supported on DGX Spark**, and mechanisms like `nvidia-peermem`, `dma-buf`, GDRCopy don’t work. ([NVIDIA Support][6])

What that means in practice:

* NCCL/RDMA can’t “NIC ↔ GPU memory” DMA directly
* collectives get **host-staged** (bounce buffers) → **CPU + DRAM bandwidth** gets dragged into your critical path
* **prefill is especially sensitive** because activation tensors are larger than steady-state decode

So your “encode” (prefill) throughput plateaus early. This also matches the symptom you saw elsewhere: **high system CPU time even when mostly idle**, because the communication path + polling is expensive.

---

## 5) You’re probably kneecapping vLLM with `--enforce-eager`

`--enforce-eager=True` **disables CUDA Graph** and forces eager PyTorch execution. vLLM’s own docs say the non-eager path uses “CUDA graph and eager execution in hybrid for maximal performance.” ([docs.vllm.ai][7])

If you can run without `--enforce-eager`, you should—especially on Spark where CPU/launch overhead matters more.

This is one of the most common reasons people get “stuck” at mediocre throughput.

---

## 6) Other config reasons you get stuck around ~1k

These don’t change physics, but they can easily leave 20–50% on the floor:

### A) **Not using `--async-scheduling`**

vLLM’s Qwen3‑VL guide: `--async-scheduling` improves overall system performance by overlapping scheduling overhead with decoding. ([docs.vllm.ai][2])
On CPU-constrained systems, this matters.

### B) **MoE routing without Expert Parallelism**

vLLM supports **Expert Parallelism (EP)** and describes it as improving locality/efficiency/throughput. ([docs.vllm.ai][8])
For a giant MoE, EP can matter a lot—*if* your GPU topology + sharding works out.

### C) **You’re actually measuring “mixed prefill+decode,” not pure encode**

vLLM V1 uses chunked prefill and explicitly co-batches **compute‑bound prefill** with **memory‑bound decode**. ([docs.vllm.ai][9])
So “prompt tok/s” can look capped if your run is dominated by decode or if decode is prioritized (which it is).

### D) **Multimodal overhead (images)**

The Qwen3‑VL recipe recommends:

* `--mm-encoder-tp-mode data` (vision encoder is small; TP adds comm overhead) ([docs.vllm.ai][2])
* choose cache strategy:

  * mostly unique images → `--mm-processor-cache-gb 0` (avoid caching overhead)
  * repeated images → `--mm-processor-cache-type shm` (shared-memory cache) ([docs.vllm.ai][2])

If you send lots of concurrent image requests, CPU preprocessing + vision encoder can become your “encode” limiter.

---

## 7) What I’d do next (highest ROI) to break out of “~1k and stuck”

No fluff—this is the shortest path to an answer:

1. **Confirm what “encode” is** using metrics:

   * sample `vllm:prompt_tokens_total` delta/sec for prefill ([docs.vllm.ai][1])
   * run one test with `--limit-mm-per-prompt.image 0 --limit-mm-per-prompt.video 0` to isolate text-only (the recipe explicitly calls this out as a mode) ([docs.vllm.ai][2])

2. **Drop `--enforce-eager`** unless you *need* it for stability/debugging. It disables CUDA graphs. ([docs.vllm.ai][7])

3. **Enable `--async-scheduling`** (it’s recommended in the Qwen3‑VL guide). ([docs.vllm.ai][2])

4. **Try `--enable-expert-parallel`** (MoE) and verify it’s actually taking effect. ([docs.vllm.ai][8])

5. If images are common:

   * ensure `--mm-encoder-tp-mode data` is set ([docs.vllm.ai][2])
   * pick the right multimodal cache mode (`shm` vs `0`) ([docs.vllm.ai][2])

6. Accept the physics:

   * Spark’s **273 GB/s unified memory** is a real cap ([NVIDIA][4])
   * cross-node TP is hampered by **no GPUDirect RDMA** ([NVIDIA Support][6])

---

## Bottom line

If you’re asking “why can’t I get way more than ~1k tok/s prefill?”: because **235B-class inference is bandwidth + interconnect dominated**, and **DGX Spark + cross-node TP (without GPUDirect RDMA) is exactly the setup that makes prefill throughput hit a low ceiling**. The one “self-inflicted” cap I’d fix first is **`--enforce-eager`** (unless it’s mandatory for your run). ([docs.vllm.ai][7])

If you paste the **exact command line** you’re running (especially whether `--enforce-eager`, `--async-scheduling`, `--enable-expert-parallel`, and your `max-model-len` are set), I can tell you which 1–2 flags are most likely responsible for the gap *in your specific case* without guessing.

[1]: https://docs.vllm.ai/en/stable/design/metrics/?utm_source=chatgpt.com "Metrics - vLLM"
[2]: https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html "Qwen3-VL Usage Guide - vLLM Recipes"
[3]: https://qwen.readthedocs.io/en/latest/getting_started/speed_benchmark.html "Speed Benchmark - Qwen"
[4]: https://www.nvidia.com/en-us/products/workstations/dgx-spark/?utm_source=chatgpt.com "NVIDIA DGX Spark"
[5]: https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/?utm_source=chatgpt.com "NVIDIA DGX Spark In-Depth Review: A New Standard for ..."
[6]: https://nvidia.custhelp.com/app/answers/detail/a_id/5780/~/is-gpudirect-rdma-supported-on-dgx-spark%3F?utm_source=chatgpt.com "Is GPUDirect RDMA supported on DGX Spark?"
[7]: https://docs.vllm.ai/en/stable/configuration/engine_args/?utm_source=chatgpt.com "Engine Arguments - vLLM"
[8]: https://docs.vllm.ai/en/latest/serving/expert_parallel_deployment/?utm_source=chatgpt.com "Expert Parallel Deployment - vLLM"
[9]: https://docs.vllm.ai/en/stable/configuration/optimization/?utm_source=chatgpt.com "Optimization and Tuning - vLLM"

