# DGX Spark 200G RoCE + vLLM multi-node Qwen3-VL-235B: lessons learned (Jan 2026)

Generated by GPT 5.2 xhigh.

This gist summarizes local notes and scripts in `/home/tom/llm` (CLAUDE.md, chatgpt_pro_answers.txt, and the shell/python helpers) from two-node DGX Spark experiments (spark-2 and spark-3). Adjust hostnames and IPs for your environment.

## TL;DR
- ConnectX-7 ports are Ethernet-only (RoCE, not IB). Each 200G QSFP port shows up as two Linux interfaces. Use IPs on both halves to get full bandwidth.
- One QSFP cable is enough; use the same port on both nodes (port 1 to port 1). Cross-cabling fails. Two cables do not give 400G in practice.
- GPUDirect RDMA is not supported on GB10. NCCL can still use NET/IB with GDR disabled; when IB fails, use socket as a safe fallback.
- vLLM + AWQ is the "works now" path for Qwen3-VL-235B on dual Spark. SGLang multi-node AWQ hangs after NCCL init.
- Required fixes: disable cuDNN for sm_121 and set `VLLM_ATTENTION_BACKEND=TRITON_ATTN` for VLM.
- RDMA vs socket: ~22 GB/s vs ~4-6 GB/s in NCCL microbenchmarks, and +79% decode throughput in our vLLM sweep at high concurrency (299 vs 167 tok/s at c=256). For low QPS, gains are mostly latency, not bandwidth.

## Hardware and cabling facts
- Each 200G QSFP port exposes two interfaces, e.g. `enp1s0f1np1` + `enP2p1s0f1np1`.
- Full 200G requires driving both halves concurrently.
- Use `ibdev2netdev` and `ethtool <iface>` to confirm link and speed.
- Spark CX-7 ports are Ethernet-only; RoCE is the supported path.

## Network setup (two-rail RoCE)
Example IPs (spark-2 <-> spark-3):
- Rail 1: `192.168.100.10/24` <-> `192.168.100.11/24`
- Rail 2: `192.168.101.10/24` <-> `192.168.101.11/24`

Netplan templates:
- `configs/40-cx7-spark2.yaml`
- `configs/40-cx7-spark3.yaml`
- `configs/90-spark-cx7-ipv6.conf` (disable IPv6 for GID stability)

Notes:
- MTU 9000 on both rails helps.
- Disable IPv6 on CX-7 interfaces to keep RoCE GID indexing stable.

## RoCE validation
Quick checks:

```bash
ibdev2netdev
ibv_devinfo -v | rg "Link Layer|transport"
ethtool enp1s0f1np1 | rg "Speed|Link detected"
ping -c 3 192.168.100.11
```

RDMA bandwidth test (single rail):

```bash
# Server
ib_write_bw -R --report_gbits -d rocep1s0f1 -q 4 -s 8388608 -D 10
# Client
ib_write_bw -R --report_gbits -d rocep1s0f1 -q 4 -s 8388608 -D 10 192.168.100.10
```

Two-rail aggregate test (run both in parallel):

```bash
# Server
ib_write_bw -R --report_gbits -d rocep1s0f1 -p 18515 &
ib_write_bw -R --report_gbits -d roceP2p1s0f1 -p 18516 &
# Client
ib_write_bw -R --report_gbits -d rocep1s0f1 -p 18515 192.168.100.10 &
ib_write_bw -R --report_gbits -d roceP2p1s0f1 -p 18516 192.168.101.10 &
```

Observed: ~106 Gb/s per rail, ~212 Gb/s aggregate.

## NCCL + container gotchas (IB with GDR disabled)
- GPUDirect RDMA is not supported on GB10. Force host-staged IB:
  - `NCCL_NET_PLUGIN=none`
  - `NCCL_DMABUF_ENABLE=0`
  - `NCCL_NET_GDR_LEVEL=LOC`
  - `NCCL_NET_GDR_C2C=0`
- Use RDMA devices and memlock in containers:
  - `--device=/dev/infiniband --ulimit memlock=-1 --cap-add=IPC_LOCK`
- Set env vars at container startup (Ray workers do not inherit later shell env).
- Verify logs show `NET/IB` not `Socket`.
- Check for `/etc/nccl.conf` overrides in containers.

Multi-rail hints:
- Prefer two subnets (multi-rail) to avoid routing issues.
- If you must stay on one subnet, consider bonding or policy routing.
- Example variables used in this repo:
  - `NCCL_IB_HCA='=rocep1s0f1:1,roceP2p1s0f1:1'`
  - `NCCL_NETDEVS_POLICY=ALL`
  - `NCCL_CROSS_NIC=0`
  - `NCCL_SOCKET_IFNAME='=enp1s0f1np1,enP2p1s0f1np1'`
- Socket fallback: `NCCL_NET=Socket` or `NCCL_IB_DISABLE=1`.
- Socket tuning (only if chasing bandwidth): `NCCL_SOCKET_NTHREADS=4`, `NCCL_NSOCKS_PERTHREAD=4`.

## vLLM multi-node recipe (spark-2 head, spark-3 worker)
Container: `nvcr.io/nvidia/vllm:25.11-py3`
Model: `QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ`

Key env vars:
- `NCCL_SOCKET_IFNAME`, `GLOO_SOCKET_IFNAME`, `UCX_NET_DEVICES`
- `RAY_memory_monitor_refresh_ms=0`
- `HF_HUB_OFFLINE=1` if DNS is flaky
- `VLLM_ATTENTION_BACKEND=TRITON_ATTN`

Required workarounds:
- GB10 lacks cuDNN conv3d kernels for sm_121. Disable cuDNN globally.
  - Mount `sitecustomize.py` into `/usr/lib/python3.12/sitecustomize.py`
  - Or use `disable_cudnn.pth` or `vllm_nocudnn.py`

Model args we use:
- `--tensor-parallel-size 2`
- `--quantization awq`
- `--kv-cache-dtype fp8`
- `--gpu-memory-utilization 0.75`
- `--limit-mm-per-prompt.video 0`
- `--enforce-eager`

Notes:
- vLLM multi-node forces Ray compiled DAG on; CPU overhead on head is expected.
- `--mm-encoder-tp-mode data` hangs during multi-node encoder profiling; keep it off.

Script: `start-vllm-multinode.sh` handles all of this.

API call (from spark-1 over 10GbE):

```bash
curl http://192.168.102.11:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ","messages":[{"role":"user","content":"Hi"}],"max_tokens":64}'
```

## Performance notes (observed)
- RDMA microbench: ~106 Gb/s per rail, ~212 Gb/s aggregate.
- vLLM throughput (Qwen3-VL-235B AWQ):
  - Socket: ~167 tok/s decode at concurrency 256
  - RDMA: ~299 tok/s decode at concurrency 256 (+79%)
- TTFT ~5s. Model load: ~3 min + encoder profiling ~5 min.
- If real traffic is only a few Gb/s, RDMA mostly reduces latency, not throughput.

## Model and memory notes
- Must use quantized weights (AWQ or NVFP4). BF16/FP8 do not fit.
- Long contexts can trigger driver OOM. 4K is safe; validate before raising.
- vLLM may auto-config large context, but treat it as "possible", not "guaranteed".
- Faster load options:
  - `--safetensors-load-strategy eager` (faster, uses more RAM)
  - `fastsafetensors` for GPU direct storage (can OOM in TP)
- Qwen-VL utilities: `pip install qwen-vl-utils==0.0.14`

## Scripts and configs in this repo
- `benchmark-rdma.sh` - runs `ib_write_bw` on both rails via ssh and prints expected results.
- `benchmark_vllm.py` - async OpenAI-style benchmark; `--sweep` for concurrency, `--prefill` for prompt length scaling.
- `start-vllm-multinode.sh` - launches Ray head/worker plus vLLM with IB envs and cuDNN disable.
- `sitecustomize.py` / `disable_cudnn.pth` / `vllm_nocudnn.py` - ways to disable cuDNN globally or for vLLM.
- `40-cx7-spark2.yaml`, `40-cx7-spark3.yaml` - minimal netplan configs.
- `configs/40-cx7-spark2.yaml`, `configs/40-cx7-spark3.yaml` - same with MTU 9000.
- `configs/90-spark-cx7-ipv6.conf` - disable IPv6 on CX-7.
- `50-10gbe-spark2.yaml` - 10GbE IP for spark-1 to access the API via `192.168.102.11`.

## References
- DGX Spark clustering guide: https://docs.nvidia.com/dgx/dgx-spark/spark-clustering.html
- NCCL env vars: https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/env.html
- vLLM distributed troubleshooting: https://docs.vllm.ai/en/stable/serving/distributed_troubleshooting/
- vLLM Qwen3-VL recipe: https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-VL.html
- TRT-LLM for Spark: https://build.nvidia.com/spark/trt-llm
- Qwen3-VL AWQ model: https://huggingface.co/QuantTrio/Qwen3-VL-235B-A22B-Instruct-AWQ
